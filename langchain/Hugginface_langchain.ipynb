{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bd6bb3b-8990-4217-b23d-ea5a32cc9628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader,PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b21101e-8465-4702-8999-80de978985b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'paper\\\\attention.pdf', 'page': 0}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'source': 'paper\\\\attention.pdf', 'page': 1}, page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [ 21] and conditional\\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\\nof continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\\nsequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       " Document(metadata={'source': 'paper\\\\attention.pdf', 'page': 2}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512 .\\nDecoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'source': 'paper\\\\attention.pdf', 'page': 3}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n√dk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1√dk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q·k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4'),\n",
       " Document(metadata={'source': 'paper\\\\attention.pdf', 'page': 4}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\\nwhere head i= Attention( QWQ\\ni, KWK\\ni, V WV\\ni)\\nWhere the projections are parameter matrices WQ\\ni∈Rdmodel×dk,WK\\ni∈Rdmodel×dk,WV\\ni∈Rdmodel×dv\\nandWO∈Rhdv×dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n•The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by√dmodel.\\n5'),\n",
       " Document(metadata={'source': 'paper\\\\attention.pdf', 'page': 5}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000 ·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'),\n",
       " Document(metadata={'source': 'paper\\\\attention.pdf', 'page': 6}, page_content='length nis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,\\norO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+n·d2). Even with k=n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [ 20] with β1= 0.9,β2= 0.98andϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate =d−0.5\\nmodel·min(step_num−0.5, step _num·warmup _steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup _steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup _steps = 4000 .\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7'),\n",
       " Document(metadata={'source': 'paper\\\\attention.pdf', 'page': 7}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModelBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0·1020\\nGNMT + RL [38] 24.6 39.92 2.3·10191.4·1020\\nConvS2S [9] 25.16 40.46 9.6·10181.5·1020\\nMoE [32] 26.03 40.56 2.0·10191.2·1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0·1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8·10201.1·1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7·10191.2·1021\\nTransformer (base model) 27.3 38.1 3.3·1018\\nTransformer (big) 28.4 41.8 2.3·1019\\nResidual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop= 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls= 0.1[36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop= 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4and length penalty α= 0.6[38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'source': 'paper\\\\attention.pdf', 'page': 8}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dvPdrop ϵlstrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'),\n",
       " Document(metadata={'source': 'paper\\\\attention.pdf', 'page': 9}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21andα= 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n10'),\n",
       " Document(metadata={'source': 'paper\\\\attention.pdf', 'page': 10}, page_content='[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL , 2016.\\n[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing , pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n11'),\n",
       " Document(metadata={'source': 'paper\\\\attention.pdf', 'page': 11}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics , 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems , 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers) , pages 434–443. ACL, August 2013.\\n12'),\n",
       " Document(metadata={'source': 'paper\\\\attention.pdf', 'page': 12}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'source': 'paper\\\\attention.pdf', 'page': 13}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
       " Document(metadata={'source': 'paper\\\\attention.pdf', 'page': 14}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15'),\n",
       " Document(metadata={'source': 'paper\\\\robert.pdf', 'page': 0}, page_content='arXiv:1907.11692v1  [cs.CL]  26 Jul 2019RoBERTa: A Robustly Optimized BERT Pretraining Approach\\nYinhan Liu∗§Myle Ott∗§Naman Goyal∗§Jingfei Du∗§Mandar Joshi†\\nDanqi Chen§Omer Levy§Mike Lewis§Luke Zettlemoyer†§Veselin Stoyanov§\\n†Paul G. Allen School of Computer Science & Engineering,\\nUniversity of Washington, Seattle, WA\\n{mandar90,lsz }@cs.washington.edu\\n§Facebook AI\\n{yinhanliu,myleott,naman,jingfeidu,\\ndanqi,omerlevy,mikelewis,lsz,ves }@fb.com\\nAbstract\\nLanguage model pretraining has led to sig-\\nniﬁcant performance gains but careful com-\\nparison between different approaches is chal-\\nlenging. Training is computationally expen-\\nsive, often done on private datasets of different\\nsizes, and, as we will show, hyperparameter\\nchoices have signiﬁcant impact on the ﬁnal re-\\nsults. We present a replication study of BERT\\npretraining ( Devlin et al. ,2019 ) that carefully\\nmeasures the impact of many key hyperparam-\\neters and training data size. We ﬁnd that BERT\\nwas signiﬁcantly undertrained, and can match\\nor exceed the performance of every model\\npublished after it. Our best model achieves\\nstate-of-the-art results on GLUE, RACE and\\nSQuAD. These results highlight the impor-\\ntance of previously overlooked design choices,\\nand raise questions about the source of re-\\ncently reported improvements. We release our\\nmodels and code.1\\n1 Introduction\\nSelf-training methods such as ELMo ( Peters et al. ,\\n2018 ), GPT ( Radford et al. ,2018 ), BERT\\n(Devlin et al. ,2019 ), XLM ( Lample and Conneau ,\\n2019 ), and XLNet ( Yang et al. ,2019 ) have\\nbrought signiﬁcant performance gains, but it can\\nbe challenging to determine which aspects of\\nthe methods contribute the most. Training is\\ncomputationally expensive, limiting the amount\\nof tuning that can be done, and is often done with\\nprivate training data of varying sizes, limiting\\nour ability to measure the effects of the modeling\\nadvances.\\n∗Equal contribution.\\n1Our models and code are available at:\\nhttps://github.com/pytorch/fairseqWe present a replication study of BERT pre-\\ntraining ( Devlin et al. ,2019 ), which includes a\\ncareful evaluation of the effects of hyperparmeter\\ntuning and training set size. We ﬁnd that BERT\\nwas signiﬁcantly undertrained and propose an im-\\nproved recipe for training BERT models, which\\nwe call RoBERTa, that can match or exceed the\\nperformance of all of the post-BERT methods.\\nOur modiﬁcations are simple, they include: (1)\\ntraining the model longer, with bigger batches,\\nover more data; (2) removing the next sentence\\nprediction objective; (3) training on longer se-\\nquences; and (4) dynamically changing the mask-\\ning pattern applied to the training data. We also\\ncollect a large new dataset (CC-N EWS) of compa-\\nrable size to other privately used datasets, to better\\ncontrol for training set size effects.\\nWhen controlling for training data, our im-\\nproved training procedure improves upon the pub-\\nlished BERT results on both GLUE and SQuAD.\\nWhen trained for longer over additional data, our\\nmodel achieves a score of 88.5 on the public\\nGLUE leaderboard, matching the 88.4 reported\\nbyYang et al. (2019 ). Our model establishes a\\nnew state-of-the-art on 4/9 of the GLUE tasks:\\nMNLI, QNLI, RTE and STS-B. We also match\\nstate-of-the-art results on SQuAD and RACE.\\nOverall, we re-establish that BERT’s masked lan-\\nguage model training objective is competitive\\nwith other recently proposed training objectives\\nsuch as perturbed autoregressive language model-\\ning (Yang et al. ,2019 ).2\\nIn summary, the contributions of this paper\\nare: (1) We present a set of important BERT de-\\nsign choices and training strategies and introduce\\n2It is possible that these other methods could also improve\\nwith more tuning. We leave this exploration to future work.'),\n",
       " Document(metadata={'source': 'paper\\\\robert.pdf', 'page': 1}, page_content='alternatives that lead to better downstream task\\nperformance; (2) We use a novel dataset, CC-\\nNEWS, and conﬁrm that using more data for pre-\\ntraining further improves performance on down-\\nstream tasks; (3) Our training improvements show\\nthat masked language model pretraining, under\\nthe right design choices, is competitive with all\\nother recently published methods. We release our\\nmodel, pretraining and ﬁne-tuning code imple-\\nmented in PyTorch ( Paszke et al. ,2017 ).\\n2 Background\\nIn this section, we give a brief overview of the\\nBERT ( Devlin et al. ,2019 ) pretraining approach\\nand some of the training choices that we will ex-\\namine experimentally in the following section.\\n2.1 Setup\\nBERT takes as input a concatenation of two\\nsegments (sequences of tokens), x1,...,x N\\nandy1,...,yM. Segments usually consist of\\nmore than one natural sentence. The two seg-\\nments are presented as a single input sequence\\nto BERT with special tokens delimiting them:\\n[CLS],x1,...,x N,[SEP],y1,...,yM,[EOS].\\nMandNare constrained such that M+N < T ,\\nwhereTis a parameter that controls the maximum\\nsequence length during training.\\nThe model is ﬁrst pretrained on a large unla-\\nbeled text corpus and subsequently ﬁnetuned us-\\ning end-task labeled data.\\n2.2 Architecture\\nBERT uses the now ubiquitous transformer archi-\\ntecture ( Vaswani et al. ,2017 ), which we will not\\nreview in detail. We use a transformer architecture\\nwithLlayers. Each block uses Aself-attention\\nheads and hidden dimension H.\\n2.3 Training Objectives\\nDuring pretraining, BERT uses two objectives:\\nmasked language modeling and next sentence pre-\\ndiction.\\nMasked Language Model (MLM) A random\\nsample of the tokens in the input sequence is\\nselected and replaced with the special token\\n[MASK]. The MLM objective is a cross-entropy\\nloss on predicting the masked tokens. BERT uni-\\nformly selects 15% of the input tokens for possi-\\nble replacement. Of the selected tokens, 80% are\\nreplaced with [MASK], 10% are left unchanged,and 10% are replaced by a randomly selected vo-\\ncabulary token.\\nIn the original implementation, random mask-\\ning and replacement is performed once in the be-\\nginning and saved for the duration of training, al-\\nthough in practice, data is duplicated so the mask\\nis not always the same for every training sentence\\n(see Section 4.1).\\nNext Sentence Prediction (NSP) NSP is a bi-\\nnary classiﬁcation loss for predicting whether two\\nsegments follow each other in the original text.\\nPositive examples are created by taking consecu-\\ntive sentences from the text corpus. Negative ex-\\namples are created by pairing segments from dif-\\nferent documents. Positive and negative examples\\nare sampled with equal probability.\\nThe NSP objective was designed to improve\\nperformance on downstream tasks, such as Natural\\nLanguage Inference ( Bowman et al. ,2015 ), which\\nrequire reasoning about the relationships between\\npairs of sentences.\\n2.4 Optimization\\nBERT is optimized with Adam ( Kingma and Ba ,\\n2015 ) using the following parameters: β1= 0.9,\\nβ2= 0.999,ǫ=1e-6 and L2weight de-\\ncay of0.01. The learning rate is warmed up\\nover the ﬁrst 10,000 steps to a peak value of\\n1e-4, and then linearly decayed. BERT trains\\nwith a dropout of 0.1 on all layers and at-\\ntention weights, and a GELU activation func-\\ntion ( Hendrycks and Gimpel ,2016 ). Models are\\npretrained for S=1,000,000 updates, with mini-\\nbatches containing B=256 sequences of maxi-\\nmum length T=512 tokens.\\n2.5 Data\\nBERT is trained on a combination of B OOK COR-\\nPUS (Zhu et al. ,2015 ) plus English W IKIPEDIA ,\\nwhich totals 16GB of uncompressed text.3\\n3 Experimental Setup\\nIn this section, we describe the experimental setup\\nfor our replication study of BERT.\\n3.1 Implementation\\nWe reimplement BERT in FAIRSEQ (Ott et al. ,\\n2019 ). We primarily follow the original BERT\\n3Yang et al. (2019 ) use the same dataset but report having\\nonly 13GB of text after data cleaning. This is most likely due\\nto subtle differences in cleaning of the Wikipedia data.'),\n",
       " Document(metadata={'source': 'paper\\\\robert.pdf', 'page': 2}, page_content='optimization hyperparameters, given in Section 2,\\nexcept for the peak learning rate and number of\\nwarmup steps, which are tuned separately for each\\nsetting. We additionally found training to be very\\nsensitive to the Adam epsilon term, and in some\\ncases we obtained better performance or improved\\nstability after tuning it. Similarly, we found setting\\nβ2= 0.98to improve stability when training with\\nlarge batch sizes.\\nWe pretrain with sequences of at most T= 512\\ntokens. Unlike Devlin et al. (2019 ), we do not ran-\\ndomly inject short sequences, and we do not train\\nwith a reduced sequence length for the ﬁrst 90% of\\nupdates. We train only with full-length sequences.\\nWe train with mixed precision ﬂoating point\\narithmetic on DGX-1 machines, each with 8 ×\\n32GB Nvidia V100 GPUs interconnected by In-\\nﬁniband ( Micikevicius et al. ,2018 ).\\n3.2 Data\\nBERT-style pretraining crucially relies on large\\nquantities of text. Baevski et al. (2019 ) demon-\\nstrate that increasing data size can result in im-\\nproved end-task performance. Several efforts\\nhave trained on datasets larger and more diverse\\nthan the original BERT ( Radford et al. ,2019 ;\\nYang et al. ,2019 ;Zellers et al. ,2019 ). Unfortu-\\nnately, not all of the additional datasets can be\\npublicly released. For our study, we focus on gath-\\nering as much data as possible for experimenta-\\ntion, allowing us to match the overall quality and\\nquantity of data as appropriate for each compari-\\nson.\\nWe consider ﬁve English-language corpora of\\nvarying sizes and domains, totaling over 160GB\\nof uncompressed text. We use the following text\\ncorpora:\\n•BOOK CORPUS (Zhu et al. ,2015 ) plus English\\nWIKIPEDIA . This is the original data used to\\ntrain BERT. (16GB).\\n•CC-N EWS, which we collected from the En-\\nglish portion of the CommonCrawl News\\ndataset ( Nagel ,2016 ). The data contains 63\\nmillion English news articles crawled between\\nSeptember 2016 and February 2019. (76GB af-\\nter ﬁltering).4\\n•OPENWEBTEXT (Gokaslan and Cohen ,2019 ),\\nan open-source recreation of the WebText cor-\\n4We usenews-please (Hamborg et al. ,2017 ) to col-\\nlect and extract CC-N EWS. CC-N EWS is similar to the R E-\\nALNEWS dataset described in Zellers et al. (2019 ).pus described in Radford et al. (2019 ). The text\\nis web content extracted from URLs shared on\\nReddit with at least three upvotes. (38GB).5\\n•STORIES , a dataset introduced in Trinh and Le\\n(2018 ) containing a subset of CommonCrawl\\ndata ﬁltered to match the story-like style of\\nWinograd schemas. (31GB).\\n3.3 Evaluation\\nFollowing previous work, we evaluate our pre-\\ntrained models on downstream tasks using the fol-\\nlowing three benchmarks.\\nGLUE The General Language Understand-\\ning Evaluation (GLUE) benchmark ( Wang et al. ,\\n2019b ) is a collection of 9 datasets for evaluating\\nnatural language understanding systems.6Tasks\\nare framed as either single-sentence classiﬁcation\\nor sentence-pair classiﬁcation tasks. The GLUE\\norganizers provide training and development data\\nsplits as well as a submission server and leader-\\nboard that allows participants to evaluate and com-\\npare their systems on private held-out test data.\\nFor the replication study in Section 4, we report\\nresults on the development sets after ﬁnetuning\\nthe pretrained models on the corresponding single-\\ntask training data (i.e., without multi-task training\\nor ensembling). Our ﬁnetuning procedure follows\\nthe original BERT paper ( Devlin et al. ,2019 ).\\nIn Section 5we additionally report test set re-\\nsults obtained from the public leaderboard. These\\nresults depend on a several task-speciﬁc modiﬁca-\\ntions, which we describe in Section 5.1.\\nSQuAD The Stanford Question Answering\\nDataset (SQuAD) provides a paragraph of context\\nand a question. The task is to answer the question\\nby extracting the relevant span from the context.\\nWe evaluate on two versions of SQuAD: V1.1\\nand V2.0 ( Rajpurkar et al. ,2016 ,2018 ). In V1.1\\nthe context always contains an answer, whereas in\\n5The authors and their afﬁliated institutions are not in any\\nway afﬁliated with the creation of the OpenWebText dataset.\\n6The datasets are: CoLA ( Warstadt et al. ,2018 ),\\nStanford Sentiment Treebank (SST) ( Socher et al. ,\\n2013 ), Microsoft Research Paragraph Corpus\\n(MRPC) ( Dolan and Brockett ,2005 ), Semantic Tex-\\ntual Similarity Benchmark (STS) ( Agirre et al. ,2007 ),\\nQuora Question Pairs (QQP) ( Iyer et al. ,2016 ), Multi-\\nGenre NLI (MNLI) ( Williams et al. ,2018 ), Question NLI\\n(QNLI) ( Rajpurkar et al. ,2016 ), Recognizing Textual\\nEntailment (RTE) ( Dagan et al. ,2006 ;Bar-Haim et al. ,\\n2006 ;Giampiccolo et al. ,2007 ;Bentivogli et al. ,2009 ) and\\nWinograd NLI (WNLI) ( Levesque et al. ,2011 ).'),\n",
       " Document(metadata={'source': 'paper\\\\robert.pdf', 'page': 3}, page_content='V2.0 some questions are not answered in the pro-\\nvided context, making the task more challenging.\\nFor SQuAD V1.1 we adopt the same span pre-\\ndiction method as BERT ( Devlin et al. ,2019 ). For\\nSQuAD V2.0, we add an additional binary classi-\\nﬁer to predict whether the question is answerable,\\nwhich we train jointly by summing the classiﬁca-\\ntion and span loss terms. During evaluation, we\\nonly predict span indices on pairs that are classi-\\nﬁed as answerable.\\nRACE The ReAding Comprehension from Ex-\\naminations (RACE) ( Lai et al. ,2017 ) task is a\\nlarge-scale reading comprehension dataset with\\nmore than 28,000 passages and nearly 100,000\\nquestions. The dataset is collected from English\\nexaminations in China, which are designed for\\nmiddle and high school students. In RACE, each\\npassage is associated with multiple questions. For\\nevery question, the task is to select one correct an-\\nswer from four options. RACE has signiﬁcantly\\nlonger context than other popular reading compre-\\nhension datasets and the proportion of questions\\nthat requires reasoning is very large.\\n4 Training Procedure Analysis\\nThis section explores and quantiﬁes which choices\\nare important for successfully pretraining BERT\\nmodels. We keep the model architecture ﬁxed.7\\nSpeciﬁcally, we begin by training BERT models\\nwith the same conﬁguration as BERT BASE (L=\\n12,H= 768 ,A= 12 , 110M params).\\n4.1 Static vs. Dynamic Masking\\nAs discussed in Section 2, BERT relies on ran-\\ndomly masking and predicting tokens. The orig-\\ninal BERT implementation performed masking\\nonce during data preprocessing, resulting in a sin-\\nglestatic mask. To avoid using the same mask for\\neach training instance in every epoch, training data\\nwas duplicated 10 times so that each sequence is\\nmasked in 10 different ways over the 40 epochs of\\ntraining. Thus, each training sequence was seen\\nwith the same mask four times during training.\\nWe compare this strategy with dynamic mask-\\ningwhere we generate the masking pattern every\\ntime we feed a sequence to the model. This be-\\ncomes crucial when pretraining for more steps or\\nwith larger datasets.\\n7Studying architectural changes, including larger archi-\\ntectures, is an important area for future work.Masking SQuAD 2.0 MNLI-m SST-2\\nreference 76.3 84.3 92.8\\nOur reimplementation:\\nstatic 78.3 84.3 92.5\\ndynamic 78.7 84.0 92.9\\nTable 1: Comparison between static and dynamic\\nmasking for BERT BASE. We report F1 for SQuAD and\\naccuracy for MNLI-m and SST-2. Reported results are\\nmedians over 5 random initializations (seeds). Refer-\\nence results are from Yang et al. (2019 ).\\nResults Table 1compares the published\\nBERT BASE results from Devlin et al. (2019 ) to our\\nreimplementation with either static or dynamic\\nmasking. We ﬁnd that our reimplementation\\nwith static masking performs similar to the\\noriginal BERT model, and dynamic masking is\\ncomparable or slightly better than static masking.\\nGiven these results and the additional efﬁciency\\nbeneﬁts of dynamic masking, we use dynamic\\nmasking in the remainder of the experiments.\\n4.2 Model Input Format and Next Sentence\\nPrediction\\nIn the original BERT pretraining procedure, the\\nmodel observes two concatenated document seg-\\nments, which are either sampled contiguously\\nfrom the same document (with p= 0.5) or from\\ndistinct documents. In addition to the masked lan-\\nguage modeling objective, the model is trained to\\npredict whether the observed document segments\\ncome from the same or distinct documents via an\\nauxiliary Next Sentence Prediction (NSP) loss.\\nThe NSP loss was hypothesized to be an impor-\\ntant factor in training the original BERT model.\\nDevlin et al. (2019 ) observe that removing NSP\\nhurts performance, with signiﬁcant performance\\ndegradation on QNLI, MNLI, and SQuAD 1.1.\\nHowever, some recent work has questioned the\\nnecessity of the NSP loss ( Lample and Conneau ,\\n2019 ;Yang et al. ,2019 ;Joshi et al. ,2019 ).\\nTo better understand this discrepancy, we com-\\npare several alternative training formats:\\n•SEGMENT -PAIR +NSP: This follows the original\\ninput format used in BERT ( Devlin et al. ,2019 ),\\nwith the NSP loss. Each input has a pair of seg-\\nments, which can each contain multiple natural\\nsentences, but the total combined length must\\nbe less than 512 tokens.'),\n",
       " Document(metadata={'source': 'paper\\\\robert.pdf', 'page': 4}, page_content='Model SQuAD 1.1/2.0 MNLI-m SST-2 RACE\\nOur reimplementation (with NSP loss):\\nSEGMENT -PAIR 90.4/78.7 84.0 92.9 64.2\\nSENTENCE -PAIR 88.7/76.2 82.9 92.1 63.0\\nOur reimplementation (without NSP loss):\\nFULL -SENTENCES 90.4/79.1 84.7 92.5 64.8\\nDOC-SENTENCES 90.6/79.7 84.7 92.7 65.6\\nBERT BASE 88.5/76.3 84.3 92.8 64.3\\nXLNet BASE (K = 7) –/81.3 85.8 92.7 66.1\\nXLNet BASE (K = 6) –/81.0 85.6 93.4 66.7\\nTable 2: Development set results for base models pretrained over B OOK CORPUS and W IKIPEDIA . All models are\\ntrained for 1M steps with a batch size of 256 sequences. We rep ort F1 for SQuAD and accuracy for MNLI-m,\\nSST-2 and RACE. Reported results are medians over ﬁve random initializations (seeds). Results for BERT BASEand\\nXLNet BASEare from Yang et al. (2019 ).\\n•SENTENCE -PAIR +NSP: Each input contains a\\npair of natural sentences , either sampled from\\na contiguous portion of one document or from\\nseparate documents. Since these inputs are sig-\\nniﬁcantly shorter than 512 tokens, we increase\\nthe batch size so that the total number of tokens\\nremains similar to SEGMENT -PAIR +NSP. We re-\\ntain the NSP loss.\\n•FULL -SENTENCES : Each input is packed with\\nfull sentences sampled contiguously from one\\nor more documents, such that the total length is\\nat most 512 tokens. Inputs may cross document\\nboundaries. When we reach the end of one doc-\\nument, we begin sampling sentences from the\\nnext document and add an extra separator token\\nbetween documents. We remove the NSP loss.\\n•DOC-SENTENCES : Inputs are constructed sim-\\nilarly to FULL -SENTENCES , except that they\\nmay not cross document boundaries. Inputs\\nsampled near the end of a document may be\\nshorter than 512 tokens, so we dynamically in-\\ncrease the batch size in these cases to achieve\\na similar number of total tokens as FULL -\\nSENTENCES . We remove the NSP loss.\\nResults Table 2shows results for the four dif-\\nferent settings. We ﬁrst compare the original\\nSEGMENT -PAIR input format from Devlin et al.\\n(2019 ) to the SENTENCE -PAIR format; both for-\\nmats retain the NSP loss, but the latter uses sin-\\ngle sentences. We ﬁnd that using individual\\nsentences hurts performance on downstream\\ntasks , which we hypothesize is because the model\\nis not able to learn long-range dependencies.We next compare training without the NSP\\nloss and training with blocks of text from a sin-\\ngle document ( DOC-SENTENCES ). We ﬁnd that\\nthis setting outperforms the originally published\\nBERT BASEresults and that removing the NSP loss\\nmatches or slightly improves downstream task\\nperformance , in contrast to Devlin et al. (2019 ).\\nIt is possible that the original BERT implementa-\\ntion may only have removed the loss term while\\nstill retaining the SEGMENT -PAIR input format.\\nFinally we ﬁnd that restricting sequences to\\ncome from a single document ( DOC-SENTENCES )\\nperforms slightly better than packing sequences\\nfrom multiple documents ( FULL -SENTENCES ).\\nHowever, because the DOC-SENTENCES format\\nresults in variable batch sizes, we use FULL -\\nSENTENCES in the remainder of our experiments\\nfor easier comparison with related work.\\n4.3 Training with large batches\\nPast work in Neural Machine Translation has\\nshown that training with very large mini-batches\\ncan both improve optimization speed and end-task\\nperformance when the learning rate is increased\\nappropriately ( Ott et al. ,2018 ). Recent work has\\nshown that BERT is also amenable to large batch\\ntraining ( You et al. ,2019 ).\\nDevlin et al. (2019 ) originally trained\\nBERT BASE for 1M steps with a batch size of\\n256 sequences. This is equivalent in computa-\\ntional cost, via gradient accumulation, to training\\nfor 125K steps with a batch size of 2K sequences,\\nor for 31K steps with a batch size of 8K.\\nIn Table 3we compare perplexity and end-'),\n",
       " Document(metadata={'source': 'paper\\\\robert.pdf', 'page': 5}, page_content='bsz steps lr ppl MNLI-m SST-2\\n256 1M 1e-4 3.99 84.7 92.7\\n2K 125K 7e-4 3.68 85.2 92.9\\n8K 31K 1e-3 3.77 84.6 92.8\\nTable 3: Perplexity on held-out training data ( ppl) and\\ndevelopment set accuracy for base models trained over\\nBOOK CORPUS and W IKIPEDIA with varying batch\\nsizes ( bsz). We tune the learning rate ( lr) for each set-\\nting. Models make the same number of passes over the\\ndata (epochs) and have the same computational cost.\\ntask performance of BERT BASE as we increase the\\nbatch size, controlling for the number of passes\\nthrough the training data. We observe that train-\\ning with large batches improves perplexity for the\\nmasked language modeling objective, as well as\\nend-task accuracy. Large batches are also easier to\\nparallelize via distributed data parallel training,8\\nand in later experiments we train with batches of\\n8K sequences.\\nNotably You et al. (2019 ) train BERT with even\\nlarger batche sizes, up to 32K sequences. We leave\\nfurther exploration of the limits of large batch\\ntraining to future work.\\n4.4 Text Encoding\\nByte-Pair Encoding (BPE) ( Sennrich et al. ,2016 )\\nis a hybrid between character- and word-level rep-\\nresentations that allows handling the large vocab-\\nularies common in natural language corpora. In-\\nstead of full words, BPE relies on subwords units,\\nwhich are extracted by performing statistical anal-\\nysis of the training corpus.\\nBPE vocabulary sizes typically range from\\n10K-100K subword units. However, unicode char-\\nacters can account for a sizeable portion of this\\nvocabulary when modeling large and diverse cor-\\npora, such as the ones considered in this work.\\nRadford et al. (2019 ) introduce a clever imple-\\nmentation of BPE that uses bytes instead of uni-\\ncode characters as the base subword units. Using\\nbytes makes it possible to learn a subword vocab-\\nulary of a modest size (50K units) that can still en-\\ncode any input text without introducing any “un-\\nknown” tokens.\\n8Large batch training can improve training efﬁciency even\\nwithout large scale parallel hardware through gradient ac-\\ncumulation , whereby gradients from multiple mini-batches\\nare accumulated locally before each optimization step. Thi s\\nfunctionality is supported natively in FAIRSEQ (Ott et al. ,\\n2019 ).The original BERT implementa-\\ntion ( Devlin et al. ,2019 ) uses a character-level\\nBPE vocabulary of size 30K, which is learned\\nafter preprocessing the input with heuristic tok-\\nenization rules. Following Radford et al. (2019 ),\\nwe instead consider training BERT with a larger\\nbyte-level BPE vocabulary containing 50K sub-\\nword units, without any additional preprocessing\\nor tokenization of the input. This adds approxi-\\nmately 15M and 20M additional parameters for\\nBERT BASEand BERT LARGE , respectively.\\nEarly experiments revealed only slight dif-\\nferences between these encodings, with the\\nRadford et al. (2019 ) BPE achieving slightly\\nworse end-task performance on some tasks. Nev-\\nertheless, we believe the advantages of a univer-\\nsal encoding scheme outweighs the minor degre-\\ndation in performance and use this encoding in\\nthe remainder of our experiments. A more de-\\ntailed comparison of these encodings is left to fu-\\nture work.\\n5 RoBERTa\\nIn the previous section we propose modiﬁcations\\nto the BERT pretraining procedure that improve\\nend-task performance. We now aggregate these\\nimprovements and evaluate their combined im-\\npact. We call this conﬁguration RoBERTa for\\nRobustly optimized BERT approach. Speciﬁ-\\ncally, RoBERTa is trained with dynamic mask-\\ning (Section 4.1),FULL -SENTENCES without NSP\\nloss (Section 4.2), large mini-batches (Section 4.3)\\nand a larger byte-level BPE (Section 4.4).\\nAdditionally, we investigate two other impor-\\ntant factors that have been under-emphasized in\\nprevious work: (1) the data used for pretraining,\\nand (2) the number of training passes through the\\ndata. For example, the recently proposed XLNet\\narchitecture ( Yang et al. ,2019 ) is pretrained us-\\ning nearly 10 times more data than the original\\nBERT ( Devlin et al. ,2019 ). It is also trained with\\na batch size eight times larger for half as many op-\\ntimization steps, thus seeing four times as many\\nsequences in pretraining compared to BERT.\\nTo help disentangle the importance of these fac-\\ntors from other modeling choices (e.g., the pre-\\ntraining objective), we begin by training RoBERTa\\nfollowing the BERT LARGE architecture ( L= 24 ,\\nH= 1024 ,A= 16 , 355M parameters). We\\npretrain for 100K steps over a comparable B OOK -\\nCORPUS plus W IKIPEDIA dataset as was used in'),\n",
       " Document(metadata={'source': 'paper\\\\robert.pdf', 'page': 6}, page_content='Model data bsz stepsSQuADMNLI-m SST-2(v1.1/2.0)\\nRoBERTa\\nwith B OOKS + W IKI 16GB 8K 100K 93.6/87.3 89.0 95.3\\n+ additional data ( §3.2) 160GB 8K 100K 94.0/87.7 89.3 95.6\\n+ pretrain longer 160GB 8K 300K 94.4/88.7 90.0 96.1\\n+ pretrain even longer 160GB 8K 500K 94.6/89.4 90.2 96.4\\nBERT LARGE\\nwith B OOKS + W IKI 13GB 256 1M 90.9/81.8 86.6 93.7\\nXLNet LARGE\\nwith B OOKS + W IKI 13GB 256 1M 94.0/87.8 88.4 94.4\\n+ additional data 126GB 2K 500K 94.5/88.8 89.8 95.6\\nTable 4: Development set results for RoBERTa as we pretrain o ver more data (16GB →160GB of text) and pretrain\\nfor longer (100K →300K→500K steps). Each row accumulates improvements from the row s above. RoBERTa\\nmatches the architecture and training objective of BERT LARGE . Results for BERT LARGE and XLNet LARGE are from\\nDevlin et al. (2019 ) and Yang et al. (2019 ), respectively. Complete results on all GLUE tasks can be fo und in the\\nAppendix.\\nDevlin et al. (2019 ). We pretrain our model using\\n1024 V100 GPUs for approximately one day.\\nResults We present our results in Table 4. When\\ncontrolling for training data, we observe that\\nRoBERTa provides a large improvement over the\\noriginally reported BERT LARGE results, reafﬁrming\\nthe importance of the design choices we explored\\nin Section 4.\\nNext, we combine this data with the three ad-\\nditional datasets described in Section 3.2. We\\ntrain RoBERTa over the combined data with the\\nsame number of training steps as before (100K).\\nIn total, we pretrain over 160GB of text. We ob-\\nserve further improvements in performance across\\nall downstream tasks, validating the importance of\\ndata size and diversity in pretraining.9\\nFinally, we pretrain RoBERTa for signiﬁcantly\\nlonger, increasing the number of pretraining steps\\nfrom 100K to 300K, and then further to 500K. We\\nagain observe signiﬁcant gains in downstream task\\nperformance, and the 300K and 500K step mod-\\nels outperform XLNet LARGE across most tasks. We\\nnote that even our longest-trained model does not\\nappear to overﬁt our data and would likely beneﬁt\\nfrom additional training.\\nIn the rest of the paper, we evaluate our best\\nRoBERTa model on the three different bench-\\nmarks: GLUE, SQuaD and RACE. Speciﬁcally\\n9Our experiments conﬂate increases in data size and di-\\nversity. We leave a more careful analysis of these two dimen-\\nsions to future work.we consider RoBERTa trained for 500K steps over\\nall ﬁve of the datasets introduced in Section 3.2.\\n5.1 GLUE Results\\nFor GLUE we consider two ﬁnetuning settings.\\nIn the ﬁrst setting ( single-task, dev ) we ﬁnetune\\nRoBERTa separately for each of the GLUE tasks,\\nusing only the training data for the correspond-\\ning task. We consider a limited hyperparameter\\nsweep for each task, with batch sizes ∈ {16,32}\\nand learning rates ∈ {1e−5,2e−5,3e−5}, with a\\nlinear warmup for the ﬁrst 6% of steps followed by\\na linear decay to 0. We ﬁnetune for 10 epochs and\\nperform early stopping based on each task’s eval-\\nuation metric on the dev set. The rest of the hyper-\\nparameters remain the same as during pretraining.\\nIn this setting, we report the median development\\nset results for each task over ﬁve random initial-\\nizations, without model ensembling.\\nIn the second setting ( ensembles, test ), we com-\\npare RoBERTa to other approaches on the test set\\nvia the GLUE leaderboard. While many submis-\\nsions to the GLUE leaderboard depend on multi-\\ntask ﬁnetuning, our submission depends only on\\nsingle-task ﬁnetuning . For RTE, STS and MRPC\\nwe found it helpful to ﬁnetune starting from the\\nMNLI single-task model, rather than the baseline\\npretrained RoBERTa. We explore a slightly wider\\nhyperparameter space, described in the Appendix,\\nand ensemble between 5 and 7 models per task.'),\n",
       " Document(metadata={'source': 'paper\\\\robert.pdf', 'page': 7}, page_content='MNLI QNLI QQP RTE SST MRPC CoLA STS WNLI Avg\\nSingle-task single models on dev\\nBERT LARGE 86.6/- 92.3 91.3 70.4 93.2 88.0 60.6 90.0 - -\\nXLNet LARGE 89.8/- 93.9 91.8 83.8 95.6 89.2 63.6 91.8 - -\\nRoBERTa 90.2/90.2 94.7 92.2 86.6 96.4 90.9 68.0 92.4 91.3 -\\nEnsembles on test (from leaderboard as of July 25, 2019)\\nALICE 88.2/87.9 95.7 90.7 83.5 95.2 92.6 68.6 91.1 80.8 86.3\\nMT-DNN 87.9/87.4 96.0 89.9 86.3 96.5 92.7 68.4 91.1 89.0 87.6\\nXLNet 90.2/89.8 98.6 90.3 86.3 96.8 93.0 67.8 91.6 90.4 88.4\\nRoBERTa 90.8/90.2 98.9 90.2 88.2 96.7 92.3 67.8 92.2 89.0 88.5\\nTable 5: Results on GLUE. All results are based on a 24-layer a rchitecture. BERT LARGE and XLNet LARGE results\\nare from Devlin et al. (2019 ) and Yang et al. (2019 ), respectively. RoBERTa results on the development set are a\\nmedian over ﬁve runs. RoBERTa results on the test set are ense mbles of single-task models. For RTE, STS and\\nMRPC we ﬁnetune starting from the MNLI model instead of the ba seline pretrained model. Averages are obtained\\nfrom the GLUE leaderboard.\\nTask-speciﬁc modiﬁcations Two of the GLUE\\ntasks require task-speciﬁc ﬁnetuning approaches\\nto achieve competitive leaderboard results.\\nQNLI : Recent submissions on the GLUE\\nleaderboard adopt a pairwise ranking formulation\\nfor the QNLI task, in which candidate answers\\nare mined from the training set and compared to\\none another, and a single (question, candidate)\\npair is classiﬁed as positive ( Liu et al. ,2019b ,a;\\nYang et al. ,2019 ). This formulation signiﬁcantly\\nsimpliﬁes the task, but is not directly comparable\\nto BERT ( Devlin et al. ,2019 ). Following recent\\nwork, we adopt the ranking approach for our test\\nsubmission, but for direct comparison with BERT\\nwe report development set results based on a pure\\nclassiﬁcation approach.\\nWNLI : We found the provided NLI-format\\ndata to be challenging to work with. Instead\\nwe use the reformatted WNLI data from Super-\\nGLUE ( Wang et al. ,2019a ), which indicates the\\nspan of the query pronoun and referent. We ﬁne-\\ntune RoBERTa using the margin ranking loss from\\nKocijan et al. (2019 ). For a given input sentence,\\nwe use spaCy ( Honnibal and Montani ,2017 ) to\\nextract additional candidate noun phrases from the\\nsentence and ﬁnetune our model so that it assigns\\nhigher scores to positive referent phrases than for\\nany of the generated negative candidate phrases.\\nOne unfortunate consequence of this formulation\\nis that we can only make use of the positive train-\\ning examples, which excludes over half of the pro-\\nvided training examples.10\\n10While we only use the provided WNLI training data, ourResults We present our results in Table 5. In the\\nﬁrst setting ( single-task, dev ), RoBERTa achieves\\nstate-of-the-art results on all 9 of the GLUE\\ntask development sets. Crucially, RoBERTa uses\\nthe same masked language modeling pretrain-\\ning objective and architecture as BERT LARGE , yet\\nconsistently outperforms both BERT LARGE and\\nXLNet LARGE . This raises questions about the rel-\\native importance of model architecture and pre-\\ntraining objective, compared to more mundane de-\\ntails like dataset size and training time that we ex-\\nplore in this work.\\nIn the second setting ( ensembles, test ), we\\nsubmit RoBERTa to the GLUE leaderboard and\\nachieve state-of-the-art results on 4 out of 9 tasks\\nand the highest average score to date. This is espe-\\ncially exciting because RoBERTa does not depend\\non multi-task ﬁnetuning, unlike most of the other\\ntop submissions. We expect future work may fur-\\nther improve these results by incorporating more\\nsophisticated multi-task ﬁnetuning procedures.\\n5.2 SQuAD Results\\nWe adopt a much simpler approach for SQuAD\\ncompared to past work. In particular, while\\nboth BERT ( Devlin et al. ,2019 ) and XL-\\nNet ( Yang et al. ,2019 ) augment their training data\\nwith additional QA datasets, we only ﬁnetune\\nRoBERTa using the provided SQuAD training\\ndata .Yang et al. (2019 ) also employed a custom\\nlayer-wise learning rate schedule to ﬁnetune\\nresults could potentially be improved by augmenting this wi th\\nadditional pronoun disambiguation datasets.'),\n",
       " Document(metadata={'source': 'paper\\\\robert.pdf', 'page': 8}, page_content='ModelSQuAD 1.1 SQuAD 2.0\\nEM F1 EM F1\\nSingle models on dev, w/o data augmentation\\nBERT LARGE 84.1 90.9 79.0 81.8\\nXLNet LARGE 89.0 94.5 86.1 88.8\\nRoBERTa 88.9 94.6 86.5 89.4\\nSingle models on test (as of July 25, 2019)\\nXLNet LARGE 86.3†89.1†\\nRoBERTa 86.8 89.8\\nXLNet + SG-Net Veriﬁer 87.0†89.9†\\nTable 6: Results on SQuAD. †indicates results that de-\\npend on additional external training data. RoBERTa\\nuses only the provided SQuAD data in both dev and\\ntest settings. BERT LARGE and XLNet LARGE results are\\nfrom Devlin et al. (2019 ) and Yang et al. (2019 ), re-\\nspectively.\\nXLNet, while we use the same learning rate for\\nall layers.\\nFor SQuAD v1.1 we follow the same ﬁnetun-\\ning procedure as Devlin et al. (2019 ). For SQuAD\\nv2.0, we additionally classify whether a given\\nquestion is answerable; we train this classiﬁer\\njointly with the span predictor by summing the\\nclassiﬁcation and span loss terms.\\nResults We present our results in Table 6. On\\nthe SQuAD v1.1 development set, RoBERTa\\nmatches the state-of-the-art set by XLNet. On the\\nSQuAD v2.0 development set, RoBERTa sets a\\nnew state-of-the-art, improving over XLNet by 0.4\\npoints (EM) and 0.6 points (F1).\\nWe also submit RoBERTa to the public SQuAD\\n2.0 leaderboard and evaluate its performance rel-\\native to other systems. Most of the top systems\\nbuild upon either BERT ( Devlin et al. ,2019 ) or\\nXLNet ( Yang et al. ,2019 ), both of which rely on\\nadditional external training data. In contrast, our\\nsubmission does not use any additional data.\\nOur single RoBERTa model outperforms all but\\none of the single model submissions, and is the\\ntop scoring system among those that do not rely\\non data augmentation.\\n5.3 RACE Results\\nIn RACE, systems are provided with a passage of\\ntext, an associated question, and four candidate an-\\nswers. Systems are required to classify which of\\nthe four candidate answers is correct.\\nWe modify RoBERTa for this task by concate-Model Accuracy Middle High\\nSingle models on test (as of July 25, 2019)\\nBERT LARGE 72.0 76.6 70.1\\nXLNet LARGE 81.7 85.4 80.2\\nRoBERTa 83.2 86.5 81.3\\nTable 7: Results on the RACE test set. BERT LARGE and\\nXLNet LARGE results are from Yang et al. (2019 ).\\nnating each candidate answer with the correspond-\\ning question and passage. We then encode each of\\nthese four sequences and pass the resulting [CLS]\\nrepresentations through a fully-connected layer,\\nwhich is used to predict the correct answer. We\\ntruncate question-answer pairs that are longer than\\n128 tokens and, if needed, the passage so that the\\ntotal length is at most 512 tokens.\\nResults on the RACE test sets are presented in\\nTable 7. RoBERTa achieves state-of-the-art results\\non both middle-school and high-school settings.\\n6 Related Work\\nPretraining methods have been designed\\nwith different training objectives, includ-\\ning language modeling ( Dai and Le ,2015 ;\\nPeters et al. ,2018 ;Howard and Ruder ,2018 ),\\nmachine translation ( McCann et al. ,2017 ), and\\nmasked language modeling ( Devlin et al. ,2019 ;\\nLample and Conneau ,2019 ). Many recent\\npapers have used a basic recipe of ﬁnetuning\\nmodels for each end task ( Howard and Ruder ,\\n2018 ;Radford et al. ,2018 ), and pretraining\\nwith some variant of a masked language model\\nobjective. However, newer methods have\\nimproved performance by multi-task ﬁne tun-\\ning ( Dong et al. ,2019 ), incorporating entity\\nembeddings ( Sun et al. ,2019 ), span predic-\\ntion ( Joshi et al. ,2019 ), and multiple variants\\nof autoregressive pretraining ( Song et al. ,2019 ;\\nChan et al. ,2019 ;Yang et al. ,2019 ). Perfor-\\nmance is also typically improved by training\\nbigger models on more data ( Devlin et al. ,\\n2019 ;Baevski et al. ,2019 ;Yang et al. ,2019 ;\\nRadford et al. ,2019 ). Our goal was to replicate,\\nsimplify, and better tune the training of BERT,\\nas a reference point for better understanding the\\nrelative performance of all of these methods.'),\n",
       " Document(metadata={'source': 'paper\\\\robert.pdf', 'page': 9}, page_content='7 Conclusion\\nWe carefully evaluate a number of design de-\\ncisions when pretraining BERT models. We\\nﬁnd that performance can be substantially im-\\nproved by training the model longer, with bigger\\nbatches over more data; removing the next sen-\\ntence prediction objective; training on longer se-\\nquences; and dynamically changing the masking\\npattern applied to the training data. Our improved\\npretraining procedure, which we call RoBERTa,\\nachieves state-of-the-art results on GLUE, RACE\\nand SQuAD, without multi-task ﬁnetuning for\\nGLUE or additional data for SQuAD. These re-\\nsults illustrate the importance of these previ-\\nously overlooked design decisions and suggest\\nthat BERT’s pretraining objective remains com-\\npetitive with recently proposed alternatives.\\nWe additionally use a novel dataset,\\nCC-N EWS, and release our models and\\ncode for pretraining and ﬁnetuning at:\\nhttps://github.com/pytorch/fairseq .\\nReferences\\nEneko Agirre, Llu’is M‘arquez, and Richard Wicen-\\ntowski, editors. 2007. Proceedings of the Fourth\\nInternational Workshop on Semantic Evaluations\\n(SemEval-2007) .\\nAlexei Baevski, Sergey Edunov, Yinhan Liu, Luke\\nZettlemoyer, and Michael Auli. 2019. Cloze-\\ndriven pretraining of self-attention networks. arXiv\\npreprint arXiv:1903.07785 .\\nRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,\\nDanilo Giampiccolo, Bernardo Magnini, and Idan\\nSzpektor. 2006. The second PASCAL recognising\\ntextual entailment challenge. In Proceedings of the\\nsecond PASCAL challenges workshop on recognis-\\ning textual entailment .\\nLuisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo\\nGiampiccolo, and Bernardo Magnini. 2009. The\\nﬁfth PASCAL recognizing textual entailment chal-\\nlenge.\\nSamuel R Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nInEmpirical Methods in Natural Language Process-\\ning (EMNLP) .\\nWilliam Chan, Nikita Kitaev, Kelvin Guu, Mitchell\\nStern, and Jakob Uszkoreit. 2019. KERMIT: Gener-\\native insertion-based modeling for sequences. arXiv\\npreprint arXiv:1906.01604 .Ido Dagan, Oren Glickman, and Bernardo Magnini.\\n2006. The PASCAL recognising textual entailment\\nchallenge. In Machine learning challenges. evalu-\\nating predictive uncertainty, visual object classiﬁca-\\ntion, and recognising tectual entailment .\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\\nsequence learning. In Advances in Neural Informa-\\ntion Processing Systems (NIPS) .\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In North American Association for Com-\\nputational Linguistics (NAACL) .\\nWilliam B Dolan and Chris Brockett. 2005. Auto-\\nmatically constructing a corpus of sentential para-\\nphrases. In Proceedings of the International Work-\\nshop on Paraphrasing .\\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei,\\nXiaodong Liu, Yu Wang, Jianfeng Gao, Ming\\nZhou, and Hsiao-Wuen Hon. 2019. Uniﬁed\\nlanguage model pre-training for natural language\\nunderstanding and generation. arXiv preprint\\narXiv:1905.03197 .\\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan,\\nand Bill Dolan. 2007. The third PASCAL recog-\\nnizing textual entailment challenge. In Proceedings\\nof the ACL-PASCAL workshop on textual entailment\\nand paraphrasing .\\nAaron Gokaslan and Vanya Cohen. 2019. Openweb-\\ntext corpus. http://web.archive.org/\\nsave/http://Skylion007.github.io/\\nOpenWebTextCorpus .\\nFelix Hamborg, Norman Meuschke, Corinna Bre-\\nitinger, and Bela Gipp. 2017. news-please: A\\ngeneric news crawler and extractor. In Proceedings\\nof the 15th International Symposium of Information\\nScience .\\nDan Hendrycks and Kevin Gimpel. 2016. Gaus-\\nsian error linear units (gelus). arXiv preprint\\narXiv:1606.08415 .\\nMatthew Honnibal and Ines Montani. 2017. spaCy 2:\\nNatural language understanding with Bloom embed-\\ndings, convolutional neural networks and incremen-\\ntal parsing. To appear.\\nJeremy Howard and Sebastian Ruder. 2018. Universal\\nlanguage model ﬁne-tuning for text classiﬁcation.\\narXiv preprint arXiv:1801.06146 .\\nShankar Iyer, Nikhil Dandekar, and Kornl Cser-\\nnai. 2016. First quora dataset release: Question\\npairs.https://data.quora.com/First-\\nQuora-Dataset-Release-Question-\\nPairs .'),\n",
       " Document(metadata={'source': 'paper\\\\robert.pdf', 'page': 10}, page_content='Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S.\\nWeld, Luke Zettlemoyer, and Omer Levy. 2019.\\nSpanBERT: Improving pre-training by repre-\\nsenting and predicting spans. arXiv preprint\\narXiv:1907.10529 .\\nDiederik Kingma and Jimmy Ba. 2015. Adam: A\\nmethod for stochastic optimization. In International\\nConference on Learning Representations (ICLR) .\\nVid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu,\\nYordan Yordanov, and Thomas Lukasiewicz. 2019.\\nA surprisingly robust trick for winograd schema\\nchallenge. arXiv preprint arXiv:1905.06290 .\\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\\nand Eduard Hovy. 2017. Race: Large-scale reading\\ncomprehension dataset from examinations. arXiv\\npreprint arXiv:1704.04683 .\\nGuillaume Lample and Alexis Conneau. 2019. Cross-\\nlingual language model pretraining. arXiv preprint\\narXiv:1901.07291 .\\nHector J Levesque, Ernest Davis, and Leora Morgen-\\nstern. 2011. The Winograd schema challenge. In\\nAAAI Spring Symposium: Logical Formalizations of\\nCommonsense Reasoning .\\nXiaodong Liu, Pengcheng He, Weizhu Chen, and\\nJianfeng Gao. 2019a. Improving multi-task deep\\nneural networks via knowledge distillation for\\nnatural language understanding. arXiv preprint\\narXiv:1904.09482 .\\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-\\nfeng Gao. 2019b. Multi-task deep neural networks\\nfor natural language understanding. arXiv preprint\\narXiv:1901.11504 .\\nBryan McCann, James Bradbury, Caiming Xiong, and\\nRichard Socher. 2017. Learned in translation: Con-\\ntextualized word vectors. In Advances in Neural In-\\nformation Processing Systems (NIPS) , pages 6297–\\n6308.\\nPaulius Micikevicius, Sharan Narang, Jonah Alben,\\nGregory Diamos, Erich Elsen, David Garcia, Boris\\nGinsburg, Michael Houston, Oleksii Kuchaiev,\\nGanesh Venkatesh, and Hao Wu. 2018. Mixed preci-\\nsion training. In International Conference on Learn-\\ning Representations .\\nSebastian Nagel. 2016. Cc-news. http:\\n//web.archive.org/save/http:\\n//commoncrawl.org/2016/10/news-\\ndataset-available .\\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\\nFan, Sam Gross, Nathan Ng, David Grangier, and\\nMichael Auli. 2019. FAIRSEQ : A fast, exten-\\nsible toolkit for sequence modeling. In North\\nAmerican Association for Computational Linguis-\\ntics (NAACL): System Demonstrations .Myle Ott, Sergey Edunov, David Grangier, and\\nMichael Auli. 2018. Scaling neural machine trans-\\nlation. In Proceedings of the Third Conference on\\nMachine Translation (WMT) .\\nAdam Paszke, Sam Gross, Soumith Chintala, Gre-\\ngory Chanan, Edward Yang, Zachary DeVito, Zem-\\ning Lin, Alban Desmaison, Luca Antiga, and Adam\\nLerer. 2017. Automatic differentiation in PyTorch.\\nInNIPS Autodiff Workshop .\\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\\nGardner, Christopher Clark, Kenton Lee, and Luke\\nZettlemoyer. 2018. Deep contextualized word repre-\\nsentations. In North American Association for Com-\\nputational Linguistics (NAACL) .\\nAlec Radford, Karthik Narasimhan, Time Salimans,\\nand Ilya Sutskever. 2018. Improving language un-\\nderstanding with unsupervised learning. Technical\\nreport, OpenAI.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\\nDario Amodei, and Ilya Sutskever. 2019. Language\\nmodels are unsupervised multitask learners. Techni-\\ncal report, OpenAI.\\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\\nKnow what you don’t know: Unanswerable ques-\\ntions for squad. In Association for Computational\\nLinguistics (ACL) .\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\\nPercy Liang. 2016. SQuAD: 100,000+ questions for\\nmachine comprehension of text. In Empirical Meth-\\nods in Natural Language Processing (EMNLP) .\\nRico Sennrich, Barry Haddow, and Alexandra Birch.\\n2016. Neural machine translation of rare words with\\nsubword units. In Association for Computational\\nLinguistics (ACL) , pages 1715–1725.\\nRichard Socher, Alex Perelygin, Jean Wu, Jason\\nChuang, Christopher D Manning, Andrew Ng, and\\nChristopher Potts. 2013. Recursive deep models\\nfor semantic compositionality over a sentiment tree-\\nbank. In Empirical Methods in Natural Language\\nProcessing (EMNLP) .\\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and\\nTie-Yan Liu. 2019. MASS: Masked sequence\\nto sequence pre-training for language generation.\\nInInternational Conference on Machine Learning\\n(ICML) .\\nYu Stephanie Sun, Shuohuan Wang, Yukun Li, Shikun\\nFeng, Xuyi Chen, Han Zhang, Xinlun Tian, Danxi-\\nang Zhu, Hao Tian, and Hua Wu. 2019. ERNIE: En-\\nhanced representation through knowledge integra-\\ntion. arXiv preprint arXiv:1904.09223 .\\nTrieu H Trinh and Quoc V Le. 2018. A simple\\nmethod for commonsense reasoning. arXiv preprint\\narXiv:1806.02847 .'),\n",
       " Document(metadata={'source': 'paper\\\\robert.pdf', 'page': 11}, page_content='Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need. In Advances in neural information pro-\\ncessing systems .\\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\\nLevy, and Samuel R. Bowman. 2019a. SuperGLUE:\\nA stickier benchmark for general-purpose language\\nunderstanding systems. arXiv preprint 1905.00537 .\\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\\nHill, Omer Levy, and Samuel R. Bowman. 2019b.\\nGLUE: A multi-task benchmark and analysis plat-\\nform for natural language understanding. In Inter-\\nnational Conference on Learning Representations\\n(ICLR) .\\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\\nman. 2018. Neural network acceptability judg-\\nments. arXiv preprint 1805.12471 .\\nAdina Williams, Nikita Nangia, and Samuel Bowman.\\n2018. A broad-coverage challenge corpus for sen-\\ntence understanding through inference. In North\\nAmerican Association for Computational Linguis-\\ntics (NAACL) .\\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\\n2019. Xlnet: Generalized autoregressive pretrain-\\ning for language understanding. arXiv preprint\\narXiv:1906.08237 .\\nYang You, Jing Li, Jonathan Hseu, Xiaodan Song,\\nJames Demmel, and Cho-Jui Hsieh. 2019. Reduc-\\ning bert pre-training time from 3 days to 76 minutes.\\narXiv preprint arXiv:1904.00962 .\\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\\nYejin Choi. 2019. Defending against neural fake\\nnews. arXiv preprint arXiv:1905.12616 .\\nYukun Zhu, Ryan Kiros, Richard Zemel, Ruslan\\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2015. Aligning books and movies:\\nTowards story-like visual explanations by watch-\\ning movies and reading books. In arXiv preprint\\narXiv:1506.06724 .\\nAppendix for “RoBERTa: A Robustly\\nOptimized BERT Pretraining Approach”\\nA Full results on GLUE\\nIn Table 8we present the full set of development\\nset results for RoBERTa. We present results for\\naLARGE conﬁguration that follows BERT LARGE ,\\nas well as a BASE conﬁguration that follows\\nBERT BASE.B Pretraining Hyperparameters\\nTable 9describes the hyperparameters for pre-\\ntraining of RoBERTa LARGE and RoBERTa BASE\\nC Finetuning Hyperparameters\\nFinetuning hyperparameters for RACE, SQuAD\\nand GLUE are given in Table 10. We select the\\nbest hyperparameter values based on the median\\nof 5 random seeds for each task.'),\n",
       " Document(metadata={'source': 'paper\\\\robert.pdf', 'page': 12}, page_content='MNLI QNLI QQP RTE SST MRPC CoLA STS\\nRoBERTa BASE\\n+ all data + 500k steps 87.6 92.8 91.9 78.7 94.8 90.2 63.6 91.2\\nRoBERTa LARGE\\nwith B OOKS + W IKI 89.0 93.9 91.9 84.5 95.3 90.2 66.3 91.6\\n+ additional data ( §3.2) 89.3 94.0 92.0 82.7 95.6 91.4 66.1 92.2\\n+ pretrain longer 300k 90.0 94.5 92.2 83.3 96.1 91.1 67.4 92.3\\n+ pretrain longer 500k 90.2 94.7 92.2 86.6 96.4 90.9 68.0 92.4\\nTable 8: Development set results on GLUE tasks for various co nﬁgurations of RoBERTa.\\nHyperparam RoBERTa LARGE RoBERTa BASE\\nNumber of Layers 24 12\\nHidden size 1024 768\\nFFN inner hidden size 4096 3072\\nAttention heads 16 12\\nAttention head size 64 64\\nDropout 0.1 0.1\\nAttention Dropout 0.1 0.1\\nWarmup Steps 30k 24k\\nPeak Learning Rate 4e-4 6e-4\\nBatch Size 8k 8k\\nWeight Decay 0.01 0.01\\nMax Steps 500k 500k\\nLearning Rate Decay Linear Linear\\nAdamǫ 1e-6 1e-6\\nAdamβ1 0.9 0.9\\nAdamβ2 0.98 0.98\\nGradient Clipping 0.0 0.0\\nTable 9: Hyperparameters for pretraining RoBERTa LARGE and RoBERTa BASE.\\nHyperparam RACE SQuAD GLUE\\nLearning Rate 1e-5 1.5e-5 {1e-5, 2e-5, 3e-5 }\\nBatch Size 16 48 {16, 32}\\nWeight Decay 0.1 0.01 0.1\\nMax Epochs 4 2 10\\nLearning Rate Decay Linear Linear Linear\\nWarmup ratio 0.06 0.06 0.06\\nTable 10: Hyperparameters for ﬁnetuning RoBERTa LARGE on RACE, SQuAD and GLUE.'),\n",
       " Document(metadata={'source': 'paper\\\\SDG.pdf', 'page': 0}, page_content=''),\n",
       " Document(metadata={'source': 'paper\\\\SDG.pdf', 'page': 1}, page_content=''),\n",
       " Document(metadata={'source': 'paper\\\\SDG.pdf', 'page': 2}, page_content='IN THE YEAR 2015, LEADERS FROM 193 COUNTRIES OF THE WORLD \\nCAME TOGETHER TO FACE THE FUTURE.\\nAnd what they saw was daunting. Famines. Drought. Wars. Plagues. Poverty. \\nNot just in some faraway place, but in their own cities and towns and villages.\\nThey knew things didn’t have to be this way. They knew we had enough  \\nfood to feed the world, but that it wasn’t getting shared. They knew there \\nwere medicines for HIV and other diseases, but they cost a lot. They knew  \\nthat earthquakes and floods were inevitable, but that the high death  \\ntolls were not. \\nThey also knew that billions of people worldwide shared their hope for a \\nbetter future.\\nSo leaders from these countries created a plan called the Sustainable \\nDevelopment Goals (SDGs). This set of 17 goals imagines a future just 15 years \\noff that would be rid of poverty and hunger, and safe from the worst effects of \\nclimate change. It’s an ambitious plan. \\nBut there’s ample evidence that we can succeed. In the past 15 years, the \\ninternational community cut extreme poverty in half. \\nNow we can finish the job.\\nThe United Nations Development Programme (UNDP) is one of the leading \\norganizations working to fulfil the SDGs by the year 2030. Present in nearly \\n170 countries and territories, we help nations make the Goals a reality.  \\nWe also champion the Goals so that people everywhere know how to  \\ndo their part. \\nUNDP is proud to continue as a leader in this global movement.\\nLearn about the Sustainable Development Goals.  What’s your Goal? '),\n",
       " Document(metadata={'source': 'paper\\\\SDG.pdf', 'page': 3}, page_content='END EXTREME POVERTY IN ALL FORMS  \\nBY 2030.\\nYes, it’s an ambitious goal—but we believe it can be done. In 2000, the \\nworld committed to halving the number of people living in extreme \\npoverty by the year 2015 and we met this goal. However, more than \\n800 million people around the world still live on less than $1.25 a day—\\nthat’s about the equivalent of the entire population of Europe living in \\nextreme poverty. Now it’s time to build on what we learned and end \\npoverty altogether. '),\n",
       " Document(metadata={'source': 'paper\\\\SDG.pdf', 'page': 4}, page_content='END HUNGER, ACHIEVE FOOD SECURITY \\nAND IMPROVED NUTRITION AND PROMOTE \\nSUSTAINABLE AGRICUL TURE\\nIn the past 20 years, hunger has dropped by almost half. Many \\ncountries that used to suffer from famine and hunger can now \\nmeet the nutritional needs of their most vulnerable people. It’s an \\nincredible accomplishment. Now we can go further and end hunger \\nand malnutrition once and for all. That means doing things such as \\npromoting sustainable agriculture and supporting small farmers. It’s a tall \\norder. But for the sake of the nearly 1 out of every 9 people on earth who \\ngo to bed hungry every night, we’ve got to try. Imagine a world where \\neveryone has access to sufficient and nutritious food all year round. \\nTogether, we can make that a reality by 2030. '),\n",
       " Document(metadata={'source': 'paper\\\\SDG.pdf', 'page': 5}, page_content='ENSURE HEAL THY LIVES AND PROMOTE \\nWELL-BEING FOR ALL AT ALL AGES\\nWe all know how important it is to be in good health. Our health affects \\neverything from how much we enjoy life to what work we can perform. \\nThat’s why there’s a Goal to make sure everyone has health coverage \\nand access to safe and effective medicines and vaccines. In the 25 \\nyears before the SDGs, we made big strides—preventable child deaths \\ndropped by more than half, and maternal mortality went down by \\nalmost as much. And yet some other numbers remain tragically high, like \\nthe fact that 6 million children die every year before their fifth birthday, \\nor that AIDS is the leading cause of death for adolescents in sub-Saharan \\nAfrica. We have the means to turn that around and make good health \\nmore than just a wish.'),\n",
       " Document(metadata={'source': 'paper\\\\SDG.pdf', 'page': 6}, page_content='ENSURE INCLUSIVE AND EQUITABLE QUALITY \\nEDUCATION AND PROMOTE LIFELONG \\nLEARNING OPPORTUNITIES FOR ALL  \\nFirst, the bad news on education. Poverty, armed conflict and other \\nemergencies keep many, many kids around the world out of school. In \\nfact, kids from the poorest households are four times more likely to be \\nout of school than those of the richest households. Now for some good \\nnews. Since 2000, there has been enormous progress on the goal to \\nprovide primary education to all children worldwide: the total enrolment \\nrate in developing regions has reached 91%. By measures in any school, \\nthat’s a good grade. Now, let’s get an even better grade for all kids, \\nand achieve the goal of universal primary and secondary education, \\naffordable vocational training, access to higher education and more.'),\n",
       " Document(metadata={'source': 'paper\\\\SDG.pdf', 'page': 7}, page_content='We can celebrate the great progress the world has made in becoming \\nmore prosperous and fair. But there’s a shadow to the celebration. In \\njust about every way, women and girls lag behind. There are still gross \\ninequalities in work and wages, lots of unpaid “women’s work” such as \\nchild care and domestic work, and discrimination in public decision-\\nmaking. But there are grounds for hope. More girls are in school now \\ncompared to in 2000. Most regions have reached gender parity in \\nprimary education. The percentage of women getting paid for their work \\nis on the rise. The Sustainable Development Goals aim to build on these \\nachievements to ensure that there is an end to discrimination against \\nwomen and girls everywhere.ACHIEVE GENDER EQUALITY AND  \\nEMPOWER ALL WOMEN AND GIRLS '),\n",
       " Document(metadata={'source': 'paper\\\\SDG.pdf', 'page': 8}, page_content='Everyone on earth should have access to safe and affordable drinking \\nwater. That’s the goal for 2030. While many people take clean drinking \\nwater and sanitation for granted, many others don’t. Water scarcity \\naffects more than 40 percent of people around the world, and that \\nnumber is projected to go even higher as a result of climate change. \\nIf we continue the path we’re on, by 2050 at least one in four people \\nare likely to be affected by recurring water shortages. But we can take \\na new path—more international cooperation, protecting wetlands \\nand rivers, sharing water-treatment technologies—that leads to \\naccomplishing this Goal. ENSURE AVAILABILITY AND SUSTAINABLE \\nMANAGEMENT OF WATER AND SANITATION \\nFOR ALL  '),\n",
       " Document(metadata={'source': 'paper\\\\SDG.pdf', 'page': 9}, page_content='ENSURE ACCESS TO AFFORDABLE, RELIABLE, \\nSUSTAINABLE AND MODERN ENERGY FOR \\nALL \\nBetween 1990 and 2010, the number of people with access to electricity \\nincreased by 1.7 billion. That’s progress to be proud of. And yet as the \\nworld’s population continues to rise, still more people will need cheap \\nenergy to light their homes and streets, use phones and computers, \\nand do their everyday business. How we get that energy is at issue; fossil \\nfuels and greenhouse gas emissions are making drastic changes in the \\nclimate, leading to big problems on every continent. Instead, we can \\nbecome more energy-efficient and invest in clean energy sources such \\nas solar and wind. That way we’ll meet electricity needs and protect the \\nenvironment. How’s that for a balancing act?'),\n",
       " Document(metadata={'source': 'paper\\\\SDG.pdf', 'page': 10}, page_content='PROMOTE SUSTAINED, INCLUSIVE AND \\nSUSTAINABLE ECONOMIC GROWTH, FULL \\nAND PRODUCTIVE EMPLOYMENT AND \\nDECENT WORK FOR ALL \\nAn important part of economic growth is that people have jobs that \\npay enough to support themselves and their families. The good news \\nis that the middle class is growing worldwide—almost tripling in size \\nin developing countries in the last 25 years, to more than a third of the \\npopulation. But today, job growth is not keeping pace with the growing \\nlabour force. Things don’t have to be that way. We can promote policies \\nthat encourage entrepreneurship and job creation. We can eradicate \\nforced labour, slavery and human trafficking. And in the end we can \\nachieve the goal of decent work for all women and men by 2030.'),\n",
       " Document(metadata={'source': 'paper\\\\SDG.pdf', 'page': 11}, page_content='BUILD RESILIENT INFRASTRUCTURE, \\nPROMOTE INCLUSIVE AND SUSTAINABLE \\nINDUSTRIALIZATION AND FOSTER \\nINNOVATION \\nTechnological progress helps us address big global challenges such as \\ncreating jobs and becoming more energy efficient. For example, the \\nworld is becoming ever more interconnected and prosperous thanks to \\nthe internet. The more connected we are, the more we can all benefit \\nfrom the wisdom and contributions of people everywhere on earth.  And \\nyet four billion people have no way of getting online, the vast majority \\nof them in developing countries. The more we invest in innovation \\nand infrastructure, the better off we’ll all be. Bridging the digital divide, \\npromoting sustainable industries, and investing in scientific research and \\ninnovation are all important ways to facilitate sustainable development.'),\n",
       " Document(metadata={'source': 'paper\\\\SDG.pdf', 'page': 12}, page_content='REDUCE INEQUALITY WITHIN AND AMONG \\nCOUNTRIES  \\nIt’s an old story: the rich get richer, and the poor get poorer. The divide \\nhas never been starker. We can and must adopt policies that create \\nopportunity for everyone, regardless of who they are or where they \\ncome from. Income inequality is a global problem that requires global \\nsolutions. That means improving the regulation of financial markets \\nand institutions, sending development aid where it is most needed \\nand helping people migrate safely so they can pursue opportunities. \\nTogether, we can now change the direction of the old story of inequality. '),\n",
       " Document(metadata={'source': 'paper\\\\SDG.pdf', 'page': 13}, page_content='MAKE CITIES AND HUMAN SETTLEMENTS \\nINCLUSIVE, SAFE, RESILIENT AND \\nSUSTAINABLE \\nIf you’re like most people, you live in a city. More than half the world’s \\npopulation now lives in cities, and that figure will go to about two-thirds \\nof humanity by the year 2050. Cities are getting bigger. In 1990 there \\nwere ten “mega-cities” with 10 million inhabitants or more. In 2014, there \\nwere 28 mega-cities, home to 453 million people. Incredible, huh? A lot \\nof people love cities; they’re centers of culture and business and life. The \\nthing is, they’re also often centers of extreme poverty. To make cities \\nsustainable for all, we can create good, affordable public housing. We \\ncan upgrade slum settlements. We can invest in public transport, create \\ngreen spaces, and get a broader range of people involved in urban \\nplanning decisions. That way, we can keep the things we love about \\ncities, and change the things we don’t.'),\n",
       " Document(metadata={'source': 'paper\\\\SDG.pdf', 'page': 14}, page_content='ENSURE SUSTAINABLE CONSUMPTION AND \\nPRODUCTION PATTERNS\\nSome people use a lot of stuff, and some people use very little—in fact, \\na big share of the world population is consuming too little to meet even \\ntheir basic needs. Instead, we can have a world where everybody gets \\nwhat they need to survive and thrive. And we can consume in a way \\nthat preserves our natural resources so that our children can enjoy them, \\nand their children and their children after that. The hard part is how to \\nachieve that goal. We can manage our natural resources more efficiently \\nand dispose of toxic waste better. Cut per capita food waste in half \\nglobally. Get businesses and consumers to reduce and recycle waste. \\nAnd help countries that have typically not consumed a lot to move \\ntowards more responsible consumption patterns.'),\n",
       " Document(metadata={'source': 'paper\\\\SDG.pdf', 'page': 15}, page_content='TAKE URGENT ACTION TO COMBAT CLIMATE \\nCHANGE AND ITS IMPACTS\\nEvery country in the world is seeing the drastic effects of climate \\nchange, some more than others. On average, the annual losses just \\nfrom earthquakes, tsunamis, tropical cyclones and flooding count in the \\nhundreds of billions of dollars. We can reduce the loss of life and property \\nby helping more vulnerable regions—such as land-locked countries \\nand island states—become more resilient. It is still possible, with the \\npolitical will and technological measures, to limit the increase in global \\nmean temperature to two degrees Celsius above pre-industrial levels—\\nand thus avoid the worst effects of climate change. The Sustainable \\nDevelopment Goals lay out a way for countries to work together to meet \\nthis urgent challenge. '),\n",
       " Document(metadata={'source': 'paper\\\\SDG.pdf', 'page': 16}, page_content='CONSERVE AND SUSTAINABLY USE THE \\nOCEANS, SEAS AND MARINE RESOURCES \\nFOR SUSTAINABLE DEVELOPMENT\\nThe oceans make human life possible. Their temperature, their chemistry, \\ntheir currents, their life forms. For one thing, more than 3 billion people \\ndepend on marine and coastal diversity for their livelihoods. But today \\nwe are seeing nearly a third of the world’s fish stocks overexploited. \\nThat’s not a sustainable way of life. Even people who live nowhere near \\nthe ocean can’t live without it. Oceans absorb about 30 percent of the \\ncarbon dioxide that humans produce; but we’re producing more carbon \\ndioxide than ever before and that makes the oceans more acidic—26% \\nmore, since the start of the industrial revolution. Our trash doesn’t help \\neither—13,000 pieces of plastic litter on every square kilometer of ocean. \\nSounds bad, right? Don’t despair! The Sustainable Development Goals \\nindicate targets for managing and protecting life below water. '),\n",
       " Document(metadata={'source': 'paper\\\\SDG.pdf', 'page': 17}, page_content='PROTECT, RESTORE AND PROMOTE \\nSUSTAINABLE USE OF TERRESTRIAL \\nECOSYSTEMS, SUSTAINABLY MANAGE \\nFORESTS, COMBAT DESERTIFICATION, AND \\nHAL T AND REVERSE LAND DEGRADATION \\nAND HAL T BIODIVERSITY LOSS \\nHumans and other animals rely on other forms of life on land for food, \\nclean air, clean water, and as a means of combatting climate change. \\nPlant life makes up 80% of the human diet. Forests, which cover 30% \\nof the Earth’s surface, help keep the air and water clean and the Earth’s \\nclimate in balance. That’s not to mention they’re home to millions of \\nanimal species. But the land and life on it are in trouble. Arable land \\nis disappearing 30 to 35 times faster than it has historically. Deserts \\nare spreading. Animal breeds are going extinct. We can turn these \\ntrends around. Fortunately, the Sustainable Development Goals aim to \\nconserve and restore the use of terrestrial ecosystems such as forests, \\nwetlands, drylands and mountains by 2030. '),\n",
       " Document(metadata={'source': 'paper\\\\SDG.pdf', 'page': 18}, page_content='PROMOTE PEACEFUL AND INCLUSIVE \\nSOCIETIES FOR SUSTAINABLE \\nDEVELOPMENT, PROVIDE ACCESS TO \\nJUSTICE FOR ALL AND BUILD EFFECTIVE, \\nACCOUNTABLE AND INCLUSIVE \\nINSTITUTIONS AT ALL LEVELS \\nHow can a country develop—how can people eat and teach and learn \\nand work and raise families—without peace? And how can a country \\nhave peace without justice, without human rights, without government \\nbased on the rule of law? Some parts of the world enjoy relative peace \\nand justice, and may come to take it for granted. Other parts seem to \\nbe plagued by armed conflict, crime, torture and exploitation, all of \\nwhich hinders their development. The goal of peace and justice is one \\nfor all countries to strive towards. The Sustainable Development Goals \\naim to reduce all forms of violence and propose that governments \\nand communities find lasting solutions to conflict and insecurity. That \\nmeans strengthening the rule of law, reducing the flow of illicit arms, \\nand bringing developing countries more into the center of institutions \\nof global governance.'),\n",
       " Document(metadata={'source': 'paper\\\\SDG.pdf', 'page': 19}, page_content='STRENGTHEN THE MEANS OF \\nIMPLEMENTATION AND REVITALIZE THE \\nGLOBAL PARTNERSHIP FOR SUSTAINABLE \\nDEVELOPMENT \\nThe Sustainable Development Goals are pretty big to-do list, don’t you \\nthink? In fact, it’s so big, you may just want to throw your hands up in the \\nair. “Forget it! Can’t be done! Why even try!” But we’ve got a lot going for \\nus. The world is more interconnected today than ever before, thanks to \\nthe internet, travel and global institutions. There’s a growing consensus \\nabout the need to work together to stop climate change. And the \\nSustainable Development Goals are no small matter either. 193 countries \\nagreed on these goals. Pretty incredible, isn’t it? 193 countries agreeing \\non anything? The final goal lays out a way for nations to work together to \\nachieve all the other Goals. '),\n",
       " Document(metadata={'source': 'paper\\\\SDG.pdf', 'page': 20}, page_content='Go shopping  Make a donation  \\nStart a fundraiser  \\nSpread the word  Visit shop.undp.org for SDG merchandise, and show off the goals you’re \\nmost passionate about. Money doesn’t just make the world go around; it’s also the most direct \\nway to reduce and eradicate all forms of poverty.\\nFundraising is a great way to raise money, create awareness, and inspire \\nothers. Plus, it’s fun!\\nSearch for @UNDP on Twitter, Facebook and Instagram, and share the \\ncontent you love.\\nTo donate or learn more about fundraising, visit undp.org/takeactionThere are many ways to show your support and help us reach \\nthe Sustainable Development Goals by 2030. Here are a few :WHA T CAN  \\nI DO TO HELP?'),\n",
       " Document(metadata={'source': 'paper\\\\SDG.pdf', 'page': 21}, page_content='NOTES'),\n",
       " Document(metadata={'source': 'paper\\\\SDG.pdf', 'page': 22}, page_content='NOTES'),\n",
       " Document(metadata={'source': 'paper\\\\SDG.pdf', 'page': 23}, page_content=''),\n",
       " Document(metadata={'source': 'paper\\\\sentence-encoder.pdf', 'page': 0}, page_content='Universal Sentence Encoder\\nDaniel Cera, Yinfei Yanga, Sheng-yi Konga, Nan Huaa, Nicole Limtiacob,\\nRhomni St. Johna, Noah Constanta, Mario Guajardo-C ´espedesa, Steve Yuanc,\\nChris Tara, Yun-Hsuan Sunga, Brian Stropea, Ray Kurzweila\\naGoogle Research\\nMountain View, CAbGoogle Research\\nNew York, NYcGoogle\\nCambridge, MA\\nAbstract\\nWe present models for encoding sentences\\ninto embedding vectors that speciﬁcally\\ntarget transfer learning to other NLP tasks.\\nThe models are efﬁcient and result in\\naccurate performance on diverse transfer\\ntasks. Two variants of the encoding mod-\\nels allow for trade-offs between accuracy\\nand compute resources. For both vari-\\nants, we investigate and report the rela-\\ntionship between model complexity, re-\\nsource consumption, the availability of\\ntransfer task training data, and task perfor-\\nmance. Comparisons are made with base-\\nlines that use word level transfer learning\\nvia pretrained word embeddings as well\\nas baselines do not use any transfer learn-\\ning. We ﬁnd that transfer learning using\\nsentence embeddings tends to outperform\\nword level transfer. With transfer learn-\\ning via sentence embeddings, we observe\\nsurprisingly good performance with min-\\nimal amounts of supervised training data\\nfor a transfer task. We obtain encourag-\\ning results on Word Embedding Associ-\\nation Tests (WEAT) targeted at detecting\\nmodel bias. Our pre-trained sentence en-\\ncoding models are made freely available\\nfor download and on TF Hub.\\n1 Introduction\\nLimited amounts of training data are available for\\nmany NLP tasks. This presents a challenge for\\ndata hungry deep learning methods. Given the\\nhigh cost of annotating supervised training data,\\nvery large training sets are usually not available\\nfor most research or industry NLP tasks. Many\\nmodels address the problem by implicitly per-\\nforming limited transfer learning through the use\\nFigure 1: Sentence similarity scores using embed-\\ndings from the universal sentence encoder.\\nof pre-trained word embeddings such as those\\nproduced by word2vec (Mikolov et al., 2013) or\\nGloVe (Pennington et al., 2014). However, recent\\nwork has demonstrated strong transfer task per-\\nformance using pre-trained sentence level embed-\\ndings (Conneau et al., 2017).\\nIn this paper, we present two models for produc-\\ning sentence embeddings that demonstrate good\\ntransfer to a number of other of other NLP tasks.\\nWe include experiments with varying amounts of\\ntransfer task training data to illustrate the relation-\\nship between transfer task performance and train-\\ning set size. We ﬁnd that our sentence embeddings\\ncan be used to obtain surprisingly good task per-\\nformance with remarkably little task speciﬁc train-\\ning data. The sentence encoding models are made\\npublicly available on TF Hub.\\nEngineering characteristics of models used for\\ntransfer learning are an important consideration.\\nWe discuss modeling trade-offs regarding mem-\\nory requirements as well as compute time on CPU\\nand GPU. Resource consumption comparisons are\\nmade for sentences of varying lengths.arXiv:1803.11175v2  [cs.CL]  12 Apr 2018'),\n",
       " Document(metadata={'source': 'paper\\\\sentence-encoder.pdf', 'page': 1}, page_content='import tensorflow_hub as hub\\nembed = hub.Module(\"https://tfhub.dev/google/\"\\n\"universal-sentence-encoder/1\")\\nembedding = embed([\\n\"The quick brown fox jumps over the lazy dog.\"])\\nListing 1: Python example code for using the\\nuniversal sentence encoder.\\n2 Model Toolkit\\nWe make available two new models for encoding\\nsentences into embedding vectors. One makes use\\nof the transformer (Vaswani et al., 2017) architec-\\nture, while the other is formulated as a deep aver-\\naging network (DAN) (Iyyer et al., 2015). Both\\nmodels are implemented in TensorFlow (Abadi\\net al., 2016) and are available to download from\\nTF Hub:1\\nhttps://tfhub.dev/google/\\nuniversal-sentence-encoder/1\\nThe models take as input English strings and\\nproduce as output a ﬁxed dimensional embedding\\nrepresentation of the string. Listing 1 provides a\\nminimal code snippet to convert a sentence into\\na tensor containing its sentence embedding. The\\nembedding tensor can be used directly or in-\\ncorporated into larger model graphs for speciﬁc\\ntasks.2\\nAs illustrated in Figure 1, the sentence embed-\\ndings can be trivially used to compute sentence\\nlevel semantic similarity scores that achieve ex-\\ncellent performance on the semantic textual sim-\\nilarity (STS) Benchmark (Cer et al., 2017). When\\nincluded within larger models, the sentence encod-\\ning models can be ﬁne tuned for speciﬁc tasks us-\\ning gradient based updates.\\n3 Encoders\\nWe introduce the model architecture for our two\\nencoding models in this section. Our two encoders\\nhave different design goals. One based on the\\ntransformer architecture targets high accuracy at\\nthe cost of greater model complexity and resource\\nconsumption. The other targets efﬁcient inference\\nwith slightly reduced accuracy.\\n1The encoding model for the DAN based encoder is al-\\nready available. The transformer based encoder will be made\\navailable at a later point.\\n2Visit https://colab.research.google.com/ to try the code\\nsnippet in Listing 1. Example code and documentation is\\navailable on the universal encoder website provided above.3.1 Transformer\\nThe transformer based sentence encoding model\\nconstructs sentence embeddings using the en-\\ncoding sub-graph of the transformer architecture\\n(Vaswani et al., 2017). This sub-graph uses at-\\ntention to compute context aware representations\\nof words in a sentence that take into account both\\nthe ordering and identity of all the other words.\\nThe context aware word representations are con-\\nverted to a ﬁxed length sentence encoding vector\\nby computing the element-wise sum of the repre-\\nsentations at each word position.3The encoder\\ntakes as input a lowercased PTB tokenized string\\nand outputs a 512 dimensional vector as the sen-\\ntence embedding.\\nThe encoding model is designed to be as gen-\\neral purpose as possible. This is accomplished\\nby using multi-task learning whereby a single\\nencoding model is used to feed multiple down-\\nstream tasks. The supported tasks include: a Skip-\\nThought like task (Kiros et al., 2015) for the un-\\nsupervised learning from arbitrary running text;\\na conversational input-response task for the in-\\nclusion of parsed conversational data (Henderson\\net al., 2017); and classiﬁcation tasks for train-\\ning on supervised data. The Skip-Thought task\\nreplaces the LSTM (Hochreiter and Schmidhu-\\nber, 1997) used in the original formulation with\\na model based on the Transformer architecture.\\nAs will be shown in the experimental results\\nbelow, the transformer based encoder achieves\\nthe best overall transfer task performance. How-\\never, this comes at the cost of compute time and\\nmemory usage scaling dramatically with sentence\\nlength.\\n3.2 Deep Averaging Network (DAN)\\nThe second encoding model makes use of a\\ndeep averaging network (DAN) (Iyyer et al.,\\n2015) whereby input embeddings for words and\\nbi-grams are ﬁrst averaged together and then\\npassed through a feedforward deep neural network\\n(DNN) to produce sentence embeddings. Simi-\\nlar to the Transformer encoder, the DAN encoder\\ntakes as input a lowercased PTB tokenized string\\nand outputs a 512 dimensional sentence embed-\\nding. The DAN encoder is trained similarly to the\\nTransformer based encoder. We make use of mul-\\n3We then divide by the square root of the length of the\\nsentence so that the differences between short sentences are\\nnot dominated by sentence length effects'),\n",
       " Document(metadata={'source': 'paper\\\\sentence-encoder.pdf', 'page': 2}, page_content='titask learning whereby a single DAN encoder is\\nused to supply sentence embeddings for multiple\\ndownstream tasks.\\nThe primary advantage of the DAN encoder is\\nthat compute time is linear in the length of the in-\\nput sequence. Similar to Iyyer et al. (2015), our re-\\nsults demonstrate that DANs achieve strong base-\\nline performance on text classiﬁcation tasks.\\n3.3 Encoder Training Data\\nUnsupervised training data for the sentence en-\\ncoding models are drawn from a variety of web\\nsources. The sources are Wikipedia, web news,\\nweb question-answer pages and discussion fo-\\nrums. We augment unsupervised learning with\\ntraining on supervised data from the Stanford Nat-\\nural Language Inference (SNLI) corpus (Bowman\\net al., 2015). Similar to the ﬁndings of Conneau\\net al. (2017), we observe that training to SNLI im-\\nproves transfer performance.\\n4 Transfer Tasks\\nThis section presents an overview of the data used\\nfor the transfer learning experiments and the Word\\nEmbedding Association Test (WEAT) data used to\\ncharacterize model bias.4Table 1 summarizes the\\nnumber of samples provided by the test portion of\\neach evaluation set and, when available, the size\\nof the dev and training data.\\nMR : Movie review snippet sentiment on a ﬁve\\nstar scale (Pang and Lee, 2005).\\nCR : Sentiment of sentences mined from cus-\\ntomer reviews (Hu and Liu, 2004).\\nSUBJ : Subjectivity of sentences from movie re-\\nviews and plot summaries (Pang and Lee, 2004).\\nMPQA : Phrase level opinion polarity from\\nnews data (Wiebe et al., 2005).\\nTREC : Fine grained question classiﬁcation\\nsourced from TREC (Li and Roth, 2002).\\nSST : Binary phrase level sentiment classiﬁca-\\ntion (Socher et al., 2013).\\nSTS Benchmark : Semantic textual similar-\\nity (STS) between sentence pairs scored by Pear-\\nson correlation with human judgments (Cer et al.,\\n2017).\\n4For the datasets MR, CR, and SUBJ, SST, and TREC we\\nuse the preparation of the data provided by Conneau et al.\\n(2017).WEAT : Word pairs from the psychology liter-\\nature on implicit association tests (IAT) that are\\nused to characterize model bias (Caliskan et al.,\\n2017).\\nDataset Train Dev Test\\nSST 67,349 872 1,821\\nSTS Bench 5,749 1,500 1,379\\nTREC 5,452 - 500\\nMR - - 10,662\\nCR - - 3,775\\nSUBJ - - 10,000\\nMPQA - - 10,606\\nTable 1: Transfer task evaluation sets\\n5 Transfer Learning Models\\nFor sentence classiﬁcation transfer tasks, the out-\\nput of the transformer and DAN sentence encoders\\nare provided to a task speciﬁc DNN. For the pair-\\nwise semantic similarity task, we directly assess\\nthe similarity of the sentence embeddings pro-\\nduced by our two encoders. As shown Eq. 1, we\\nﬁrst compute the cosine similarity of the two sen-\\ntence embeddings and then use arccos to convert\\nthe cosine similarity into an angular distance.5\\nsim(u,v) =(\\n1−arccos(u·v\\n||u||||v||)\\n/π)\\n(1)\\n5.1 Baselines\\nFor each transfer task, we include baselines that\\nonly make use of word level transfer and baselines\\nthat make use of no transfer learning at all. For\\nword level transfer, we use word embeddings from\\na word2vec skip-gram model trained on a corpus\\nof news data (Mikolov et al., 2013). The pre-\\ntrained word embeddings are included as input to\\ntwo model types: a convolutional neural network\\nmodels (CNN) (Kim, 2014); a DAN. The base-\\nlines that use pretrained word embeddings allow\\nus to contrast word versus sentence level trans-\\nfer. Additional baseline CNN and DAN models\\nare trained without using any pretrained word or\\nsentence embeddings.\\n5.2 Combined Transfer Models\\nWe explore combining the sentence and word level\\ntransfer models by concatenating their representa-\\ntions prior to feeding the combined representation\\n5We ﬁnd that using a similarity based on angular distance\\nperforms better on average than raw cosine similarity.'),\n",
       " Document(metadata={'source': 'paper\\\\sentence-encoder.pdf', 'page': 3}, page_content='Model MR CR SUBJ MPQA TREC SSTSTS Bench\\n(dev / test)\\nSentence & Word Embedding Transfer Learning\\nUSE D+DAN (w2v w.e.) 77.11 81.71 93.12 87.01 94.72 82.14 –\\nUSE D+CNN (w2v w.e.) 78.20 82.04 93.24 85.87 97.67 85.29 –\\nUSE T+DAN (w2v w.e.) 81.32 86.66 93.90 88.14 95.51 86.62 –\\nUSE T+CNN (w2v w.e.) 81.18 87.45 93.58 87.32 98.07 86.69 –\\nSentence Embedding Transfer Learning\\nUSE D 74.45 80.97 92.65 85.38 91.19 77.62 0.763 / 0.719 (r)\\nUSE T 81.44 87.43 93.87 86.98 92.51 85.38 0.814 / 0.782 (r)\\nUSE D+DAN (lrn w.e.) 77.57 81.93 92.91 85.97 95.86 83.41 –\\nUSE D+CNN (lrn w.e.) 78.49 81.49 92.99 85.53 97.71 85.27 –\\nUSE T+DAN (lrn w.e.) 81.36 86.08 93.66 87.14 96.60 86.24 –\\nUSE T+CNN (lrn w.e.) 81.59 86.45 93.36 86.85 97.44 87.21 –\\nWord Embedding Transfer Learning\\nDAN (w2v w.e.) 74.75 75.24 90.80 81.25 85.69 80.24 –\\nCNN (w2v w.e.) 75.10 80.18 90.84 81.38 97.32 83.74 –\\nBaselines with No Transfer Learning\\nDAN (lrn w.e.) 75.97 76.91 89.49 80.93 93.88 81.52 –\\nCNN (lrn w.e.) 76.39 79.39 91.18 82.20 95.82 84.90 –\\nTable 2: Model performance on transfer tasks. USE Tis the universal sentence encoder (USE) using\\nTransformer. USE Dis the universal encoder DAN model. Models tagged with w2v w.e. make use of\\npre-training word2vec skip-gram embeddings for the transfer task model, while models tagged with lrn\\nw.e.use randomly initialized word embeddings that are learned only on the transfer task data. Accuracy\\nis reported for all evaluations except STS Bench where we report the Pearson correlation of the similar-\\nity scores with human judgments. Pairwise similarity scores are computed directly using the sentence\\nembeddings from the universal sentence encoder as in Eq. (1).\\nto the transfer task classiﬁcation layers. For com-\\npleteness, we also explore concatenating the rep-\\nresentations from sentence level transfer models\\nwith the baseline models that do not make use of\\nword level transfer learning.\\n6 Experiments\\nTransfer task model hyperparamaters are tuned\\nusing a combination of Vizier (Golovin et al.)\\nand light manual tuning. When available, model\\nhyperparameters are tuned using task dev sets.\\nOtherwise, hyperparameters are tuned by cross-\\nvalidation on the task training data when available\\nor the evaluation test data when neither training\\nnor dev data are provided. Training repeats ten\\ntimes for each transfer task model with different\\nrandomly initialized weights and we report evalu-\\nation results by averaging across runs.\\nTransfer learning is critically important when\\ntraining data for a target task is limited. We ex-\\nplore the impact on task performance of varying\\nthe amount of training data available for the task\\nboth with and without the use of transfer learning.\\nContrasting the transformer and DAN based en-\\ncoders, we demonstrate trade-offs in model com-\\nplexity and the amount of data required to reach a\\ndesired level of accuracy on a task.To assess bias in our encoding models, we eval-\\nuate the strength of various associations learned\\nby our model on WEAT word lists. We compare\\nour result to those of Caliskan et al. (2017) who\\ndiscovered that word embeddings could be used to\\nreproduce human performance on implicit associ-\\nation tasks for both benign and potentially unde-\\nsirable associations.\\n7 Results\\nTransfer task performance is summarized in Ta-\\nble 2. We observe that transfer learning from the\\ntransformer based sentence encoder usually per-\\nforms as good or better than transfer learning from\\nthe DAN encoder. Hoewver, transfer learning us-\\ning the simpler and fast DAN encoder can for\\nsome tasks perform as well or better than the more\\nsophisticated transformer encoder. Models that\\nmake use of sentence level transfer learning tend\\nto perform better than models that only use word\\nlevel transfer. The best performance on most tasks\\nis obtained by models that make use of both sen-\\ntence and word level transfer.\\nTable 3 illustrates transfer task performance for\\nvarying amounts of training data. We observe that,\\nfor smaller quantities of data, sentence level trans-\\nfer learning can achieve surprisingly good task'),\n",
       " Document(metadata={'source': 'paper\\\\sentence-encoder.pdf', 'page': 4}, page_content='Model SST 1k SST 2k SST 4k SST 8k SST 16k SST 32k SST 67.3k\\nSentence & Word Embedding Transfer Learning\\nUSE D+DNN (w2v w.e.) 78.65 78.68 79.07 81.69 81.14 81.47 82.14\\nUSE D+CNN (w2v w.e.) 77.79 79.19 79.75 82.32 82.70 83.56 85.29\\nUSE T+DNN (w2v w.e.) 85.24 84.75 85.05 86.48 86.44 86.38 86.62\\nUSE T+CNN (w2v w.e.) 84.44 84.16 84.77 85.70 85.22 86.38 86.69\\nSentence Embedding Transfer Learning\\nUSE D 77.47 76.38 77.39 79.02 78.38 77.79 77.62\\nUSE T 84.85 84.25 85.18 85.63 85.83 85.59 85.38\\nUSE D+DNN (lrn w.e.) 75.90 78.68 79.01 82.31 82.31 82.14 83.41\\nUSE D+CNN (lrn w.e.) 77.28 77.74 79.84 81.83 82.64 84.24 85.27\\nUSE T+DNN (lrn w.e.) 84.51 84.87 84.55 85.96 85.62 85.86 86.24\\nUSE T+CNN (lrn w.e.) 82.66 83.73 84.23 85.74 86.06 86.97 87.21\\nWord Embedding Transfer Learning\\nDNN (w2v w.e.) 66.34 69.67 73.03 77.42 78.29 79.81 80.24\\nCNN (w2v w.e.) 68.10 71.80 74.91 78.86 80.83 81.98 83.74\\nBaselines with No Transfer Learning\\nDNN (lrn w.e.) 66.87 71.23 73.70 77.85 78.07 80.15 81.52\\nCNN (lrn w.e.) 67.98 71.81 74.90 79.14 81.04 82.72 84.90\\nTable 3: Task performance on SST for varying amounts of training data. SST 67.3k represents the full\\ntraining set. Using only 1,000 examples for training, transfer learning from USE T is able to obtain\\nperformance that rivals many of the other models trained on the full 67.3 thousand example training set.\\nperformance. As the training set size increases,\\nmodels that do not make use of transfer learning\\napproach the performance of the other models.\\nTable 4 contrasts Caliskan et al. (2017)’s ﬁnd-\\nings on bias within GloVe embeddings with the\\nDAN variant of the universal encoder. Similar\\nto GloVe, our model reproduces human associa-\\ntions between ﬂowers vs. insects and pleasantness\\nvs. unpleasantness. However, our model demon-\\nstrates weaker associations than GloVe for probes\\ntargeted at revealing at ageism, racism and sex-\\nism.6The differences in word association patterns\\ncan be attributed to differences in the training data\\ncomposition and the mixture of tasks used to train\\nthe sentence embeddings.\\n7.1 Discussion\\nTransfer learning leads to performance improve-\\nments on many tasks. Using transfer learning is\\nmore critical when less training data is available.\\nWhen task performance is close, the correct mod-\\neling choice should take into account engineer-\\ning trade-offs regarding the memory and compute\\n6Researchers and developers are strongly encouraged to\\nindependently verify whether biases in their overall model\\nor model components impacts their use case. For resources\\non ML fairness visit https://developers.google.com/machine-\\nlearning/fairness-overview/.resource requirements introduced by the different\\nmodels that could be used.\\n8 Resource Usage\\nThis section describes memory and compute re-\\nsource usage for the transformer and DAN sen-\\ntence encoding models for different sentence\\nlengths. Figure 2 plots model resource usage\\nagainst sentence length.\\nCompute Usage The transformer model time\\ncomplexity is O(n2)in sentence length, while the\\nDAN model is O(n). As seen in Figure 2 (a-b), for\\nshort sentences, the transformer encoding model\\nis only moderately slower than the much simpler\\nDAN model. However, compute time for trans-\\nformer increases noticeably as sentence length in-\\ncreases. In contrast, the compute time for the DAN\\nmodel stays nearly constant as sentence length is\\nincreased. Since the DAN model is remarkably\\ncomputational efﬁcient, using GPUs over CPUs\\nwill often have a much larger practical impact for\\nthe transformer based encoder.\\nMemory Usage The transformer model space\\ncomplexity also scales quadratically, O(n2), in\\nsentence length, while the DAN model space com-\\nplexity is constant in the length of the sentence.'),\n",
       " Document(metadata={'source': 'paper\\\\sentence-encoder.pdf', 'page': 5}, page_content='(a) CPU Time vs. Sentence Length\\n (b) GPU Time vs. Sentence Length\\n (c) Memory vs. Sentence Length\\nFigure 2: Model Resource Usage for both USE D and USE T at different batch sizes and sentence\\nlengths.\\nTarget words Attrib. words RefGloVe Uni. Enc. (DAN)\\nd p d p\\nEur.-American vs Afr.-American names Pleasant vs. Unpleasant 1 a 1.41 10−80.361 0.035\\nEur.-American vs. Afr.-American names Pleasant vs. Unpleasant from (a) b 1.50 10−4-0.372 0.87\\nEur.-American vs. Afr.-American names Pleasant vs. Unpleasant from (c) b 1.28 10−30.721 0.015\\nMale vs. female names Career vs family c 1.81 10−30.0248 0.48\\nMath vs. arts Male vs. female terms c 1.06 0.018 0.588 0.12\\nScience vs. arts Male vs female terms d 1.24 10−20.236 0.32\\nMental vs. physical disease Temporary vs permanent e 1.38 10−21.60 0.0027\\nYoung vs old peoples names Pleasant vs unpleasant c 1.21 10−21.01 0.022\\nFlowers vs. insects Pleasant vs. Unpleasant a 1.50 10−71.38 10−7\\nInstruments vs. Weapons Pleasant vs Unpleasant a 1.53 10−71.44 10−7\\nTable 4: Word Embedding Association Tests (WEAT) for GloVe and the Universal Encoder. Effect size\\nis reported as Cohen’s d over the mean cosine similarity scores across grouped attribute words. Statistical\\nsigniﬁcance is reported for 1 tailed p-scores. The letters in the Refcolumn indicates the source of the IAT\\nword lists: (a)Greenwald et al. (1998) (b)Bertrand and Mullainathan (2004) (c)Nosek et al. (2002a)\\n(d)Nosek et al. (2002b) (e)Monteith and Pettit (2011).\\nSimilar to compute usage, memory usage for the\\ntransformer model increases quickly with sentence\\nlength, while the memory usage for the DAN\\nmodel remains constant. We note that, for the\\nDAN model, memory usage is dominated by the\\nparameters used to store the model unigram and\\nbigram embeddings. Since the transformer model\\nonly needs to store unigram embeddings, for short\\nsequences it requires nearly half as much memory\\nas the DAN model.\\n9 Conclusion\\nBoth the transformer and DAN based universal en-\\ncoding models provide sentence level embeddings\\nthat demonstrate strong transfer performance on a\\nnumber of NLP tasks. The sentence level embed-\\ndings surpass the performance of transfer learn-\\ning using word level embeddings alone. Models\\nthat make use of sentence and word level transfer\\nachieve the best overall performance. We observe\\nthat transfer learning is most helpful when limitedtraining data is available for the transfer task. The\\nencoding models make different trade-offs regard-\\ning accuracy and model complexity that should\\nbe considered when choosing the best model for\\na particular application. The pre-trained encod-\\ning models will be made publicly available for\\nresearch and use in applications that can beneﬁt\\nfrom a better understanding of natural language.\\nAcknowledgments\\nWe thank our teammates from Descartes, Ai.h and\\nother Google groups for their feedback and sug-\\ngestions. Special thanks goes to Ben Packer and\\nYoni Halpern for implementing the WEAT assess-\\nments and discussions on model bias.\\nReferences\\nMart ´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng\\nChen, Andy Davis, Jeffrey Dean, Matthieu Devin,\\nSanjay Ghemawat, Geoffrey Irving, Michael Isard,'),\n",
       " Document(metadata={'source': 'paper\\\\sentence-encoder.pdf', 'page': 6}, page_content='Manjunath Kudlur, Josh Levenberg, Rajat Monga,\\nSherry Moore, Derek G. Murray, Benoit Steiner,\\nPaul Tucker, Vijay Vasudevan, Pete Warden, Martin\\nWicke, Yuan Yu, and Xiaoqiang Zheng. 2016. Ten-\\nsorﬂow: A system for large-scale machine learning.\\nInProceedings of USENIX OSDI’16 .\\nMarianne Bertrand and Sendhil Mullainathan. 2004.\\nAre emily and greg more employable than lakisha\\nand jamal? a ﬁeld experiment on labor market\\ndiscrimination. The American Economic Review ,\\n94(4).\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nInProceedings of EMNLP .\\nAylin Caliskan, Joanna J. Bryson, and Arvind\\nNarayanan. 2017. Semantics derived automatically\\nfrom language corpora contain human-like biases.\\nScience , 356(6334):183–186.\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\\nGazpio, and Lucia Specia. 2017. Semeval-2017\\ntask 1: Semantic textual similarity multilingual and\\ncrosslingual focused evaluation. In Proceedings of\\nSemEval-2017 .\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Loic\\nBarrault, and Antoine Bordes. 2017. Supervised\\nlearning of universal sentence representations from\\nnatural language inference data. arXiv preprint\\narXiv:1705.02364 .\\nDaniel Golovin, Benjamin Solnik, Subhodeep Moitra,\\nGreg Kochanski, John Karro, and D. Sculley.\\nGoogle vizier: A service for black-box optimization.\\nInProceedings of KDD ’17 .\\nAnthony G. Greenwald, Debbie E. McGhee, and Jor-\\ndan L. K. Schwartz. 1998. Measuring individual\\ndifferences in implicit cognition: the implicit asso-\\nciation test. Journal of personality and social psy-\\nchology , 74(6).\\nMatthew Henderson, Rami Al-Rfou, Brian Strope,\\nYun-Hsuan Sung, L ´aszl´o Luk ´acs, Ruiqi Guo, San-\\njiv Kumar, Balint Miklos, and Ray Kurzweil. 2017.\\nEfﬁcient natural language response suggestion for\\nsmart reply. CoRR , abs/1705.00652.\\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997. Long\\nshort-term memory. Neural Comput. , 9(8):1735–\\n1780.\\nMinqing Hu and Bing Liu. 2004. Mining and sum-\\nmarizing customer reviews. In Proceedings of KDD\\n’04.\\nMohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,\\nand Hal Daum ´e III. 2015. Deep unordered compo-\\nsition rivals syntactic methods for text classiﬁcation.\\nInProceedings of ACL/IJCNLP .\\nYoon Kim. 2014. Convolutional neural networks for\\nsentence classiﬁcation. In Proceedings of EMNLP .Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2015. Skip-thought vectors. In In\\nProceedings of NIPS .\\nXin Li and Dan Roth. 2002. Learning question classi-\\nﬁers. In Proceedings of COLING ’02 .\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-\\nrado, and Jeffrey Dean. 2013. Distributed represen-\\ntations of words and phrases and their composition-\\nality. In Proceedings of NIPS’13 .\\nLindsey L. Monteith and Jeremy W. Pettit. 2011. Im-\\nplicit and explicit stigmatizing attitudes and stereo-\\ntypes about depression. Journal of Social and Clin-\\nical Psychology , 30(5).\\nBrian A. Nosek, Mahzarin R. Banaji, and Anthony G.\\nGreenwald. 2002a. Harvesting implicit group at-\\ntitudes and beliefs from a demonstration web site.\\nGroup Dynamics , 6(1).\\nBrian A. Nosek, Mahzarin R. Banaji, and Anthony G\\nGreenwald. 2002b. Math = male, me = female,\\ntherefore math me. Journal of Personality and So-\\ncial Psychology, , 83(1).\\nBo Pang and Lillian Lee. 2004. A sentimental educa-\\ntion: Sentiment analysis using subjectivity summa-\\nrization based on minimum cuts. In Proceedings of\\nthe 42nd Meeting of the Association for Computa-\\ntional Linguistics (ACL’04), Main Volume .\\nBo Pang and Lillian Lee. 2005. Seeing stars: Ex-\\nploiting class relationships for sentiment categoriza-\\ntion with respect to rating scales. In Proceedings of\\nACL’05 .\\nJeffrey Pennington, Richard Socher, and Christo-\\npher D. Manning. 2014. Glove: Global vectors for\\nword representation. In Proceeding of EMNLP .\\nRichard Socher, Alex Perelygin, Jean Wu, Jason\\nChuang, Christopher D. Manning, Andrew Ng, and\\nChristopher Potts. 2013. Recursive deep models\\nfor semantic compositionality over a sentiment tree-\\nbank. In Proceedings of EMNLP .\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need. In Proceedings of NIPS .\\nJanyce Wiebe, Theresa Wilson, and Claire Cardie.\\n2005. Annotating expressions of opinions and emo-\\ntions in language. Language Resources and Evalu-\\nation , 39(2):165–210.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyPDFDirectoryLoader(\"paper/\")\n",
    "documents = loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8402c4fd-2e57-46f8-8ea3-ceb8a6ba27a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "final_document = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c3c10e6-ffbd-40e9-bb51-562ac2d2dbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\gen_ai_nlp\\Langchain\\venv\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "## Embedding Using Huggingface\n",
    "huggingface_embeddings=HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",      #sentence-transformers/all-MiniLM-l6-v2\n",
    "    model_kwargs={'device':'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings':True}\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40a738b8-2b4b-4f98-bc0a-3ef52b8ca4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.11435254e-02  1.12881670e-02 -4.82023098e-02 -3.65781151e-02\n",
      " -8.84164125e-03  2.74901595e-02  5.55983605e-03  1.57355685e-02\n",
      "  6.67438582e-02 -5.19652618e-03  1.07394194e-03 -2.29805727e-02\n",
      "  3.89843956e-02  8.15375373e-02  2.59759314e-02  1.12256631e-02\n",
      " -3.63948904e-02  4.50761989e-02  5.00951111e-02 -4.60564941e-02\n",
      "  4.19073589e-02 -3.62798870e-02 -5.30159753e-03  9.43684019e-03\n",
      " -1.14307776e-02 -3.25555503e-02 -1.22781927e-02 -3.20052616e-02\n",
      " -4.57952395e-02 -2.32240140e-01  1.70852281e-02 -3.34782898e-02\n",
      "  7.46601820e-02  1.71358362e-02 -2.87839230e-02 -2.62469184e-02\n",
      " -5.89441210e-02 -4.59725782e-02 -6.23235032e-02 -1.15564102e-02\n",
      "  1.20267980e-02  2.43485980e-02 -1.76616460e-02 -3.62666883e-02\n",
      " -2.98957098e-02 -3.75974216e-02 -1.93930101e-02 -7.48388916e-02\n",
      " -3.85977365e-02 -1.19003141e-02 -4.76373062e-02 -1.20431213e-02\n",
      " -2.57740971e-02  8.40908736e-02  7.88563583e-03 -2.14243233e-02\n",
      "  2.86728125e-02  3.72647122e-02  6.03902079e-02 -1.17800534e-02\n",
      "  1.38523495e-02  6.23644516e-02 -1.15688324e-01  7.27724843e-03\n",
      "  2.66880766e-02  4.48724180e-02 -2.10000630e-02  4.11085784e-02\n",
      " -1.03098042e-02  5.20947985e-02 -1.63232759e-02 -2.80650822e-03\n",
      " -2.02734605e-03 -1.05347605e-02  4.56047524e-03  9.07912012e-03\n",
      " -3.51946987e-03  1.89099666e-02  5.34857847e-02 -5.25371842e-02\n",
      "  9.07654166e-02  4.02582437e-03  1.63522270e-02 -9.80281159e-02\n",
      "  1.53203392e-02 -1.53583079e-03 -3.47819217e-02 -3.79722677e-02\n",
      " -4.23919186e-02 -8.07516370e-03 -2.14370657e-02 -2.36240122e-02\n",
      "  2.05237847e-02 -4.29472886e-03 -3.51274684e-02  1.02784093e-02\n",
      "  2.08758395e-02  3.14188947e-04  1.35089755e-02  4.14853662e-01\n",
      "  9.10979137e-03 -1.57090221e-02  4.83989716e-02 -1.81721672e-02\n",
      "  1.26244593e-02 -1.62450708e-02 -2.12265318e-03  1.89394988e-02\n",
      " -5.78508526e-02  5.85470498e-02 -4.83228303e-02 -1.43796252e-02\n",
      "  2.62961676e-03 -7.93511700e-03  1.95335783e-02 -1.39855240e-02\n",
      "  7.58176148e-02  2.98761595e-02 -7.30480347e-03 -4.16186899e-02\n",
      " -1.92983840e-02  2.96265017e-02 -4.16861698e-02 -3.93166393e-02\n",
      "  4.13720356e-03 -3.50625105e-02 -9.63134971e-03  1.08035505e-01\n",
      "  7.81199485e-02 -7.16480613e-03  6.09876625e-02  3.53993736e-02\n",
      " -4.58164811e-02  3.30046788e-02 -7.79352104e-03  3.26127447e-02\n",
      "  2.75136195e-02 -6.39992282e-02 -4.93003204e-02 -2.41084788e-02\n",
      "  8.93113087e-04  3.02502271e-02  1.62449908e-02  9.72385146e-03\n",
      " -7.76119530e-02  1.03129514e-01 -1.00310855e-02 -4.70319912e-02\n",
      " -2.76964512e-02  6.46815728e-03  3.46821062e-02  3.88121642e-02\n",
      "  2.22053025e-02 -3.92453335e-02  1.08177485e-02  7.28712082e-02\n",
      " -2.05327873e-03 -8.32410380e-02 -4.82329503e-02 -6.02515740e-03\n",
      " -5.91953471e-02 -5.57459751e-03 -4.32517305e-02  6.69533387e-02\n",
      "  5.08006476e-02 -4.27388139e-02 -1.17736394e-02 -4.73159291e-02\n",
      " -2.40111165e-03 -5.20024858e-02  4.75395508e-02 -2.51399074e-03\n",
      "  1.70862209e-02  4.94311713e-02 -3.61763425e-02  5.09569459e-02\n",
      " -6.67742267e-02 -3.86609286e-02  3.08255502e-03  3.87949385e-02\n",
      "  2.72825677e-02 -6.50948733e-02 -1.84251573e-02  2.47158986e-02\n",
      "  2.67452989e-02  1.58703551e-02  3.33650298e-02 -2.65145637e-02\n",
      " -2.19787359e-02 -2.64118542e-03 -3.68788652e-02  2.56238524e-02\n",
      " -2.31859051e-02  4.04255651e-02  1.01255830e-02 -2.35882811e-02\n",
      "  5.21941762e-03  3.26740704e-02 -3.22919823e-02  4.54353075e-03\n",
      " -1.95926260e-02  2.95512974e-02 -1.16591705e-02 -5.68363294e-02\n",
      " -6.37735128e-02 -8.00201017e-03 -3.93076427e-02  1.07900770e-02\n",
      "  2.70242095e-02  3.23726386e-02 -2.74970499e-03 -1.74825899e-02\n",
      "  1.83950495e-02  6.81044185e-04 -2.52260286e-02 -5.25479787e-04\n",
      " -1.83397979e-02  1.81546081e-02  4.12356816e-02  4.91407216e-02\n",
      "  5.84006170e-03 -5.17977774e-02 -7.28221051e-03 -3.19781691e-01\n",
      " -3.80907170e-02  4.07325402e-02 -6.82899123e-03  3.41452323e-02\n",
      " -1.12807289e-01  2.60146018e-02  1.95389520e-02  7.87392631e-02\n",
      "  5.26172221e-02  8.83236434e-03  1.14445193e-02 -3.87043804e-02\n",
      " -4.23565432e-02  1.31850236e-03  3.47102992e-02  1.52543175e-03\n",
      "  6.08332781e-03 -3.24292406e-02 -1.00340648e-02  9.24010947e-03\n",
      "  6.29685703e-04  2.86313295e-02 -6.93126917e-02  3.03532463e-02\n",
      "  7.99326040e-03  1.20530039e-01  3.80129670e-03  7.91948661e-02\n",
      "  2.62860842e-02 -1.75727289e-02  4.67717722e-02  1.25393760e-03\n",
      " -9.19625163e-03  3.53231244e-02 -2.95119286e-02  3.30128931e-02\n",
      " -2.46929862e-02  2.32952344e-03 -2.40747090e-02 -5.74194081e-02\n",
      " -3.01939361e-02  9.91226640e-03 -4.94275950e-02 -5.18990420e-02\n",
      " -2.30824901e-03 -3.86991464e-02 -2.56776810e-03  3.26755233e-02\n",
      "  4.82069328e-02  4.61812280e-02 -2.54944731e-02  2.32889093e-02\n",
      " -2.69336179e-02 -5.24211526e-02 -3.43990396e-03 -6.49298728e-02\n",
      " -1.63633712e-02 -8.38659629e-02  1.76495779e-02 -3.69882351e-03\n",
      " -2.53097955e-02 -4.39106748e-02 -1.62507524e-03 -2.03489116e-03\n",
      "  1.60108227e-02  5.62716536e-02  1.28808673e-02  1.17578749e-02\n",
      "  3.33411843e-02  8.99435545e-04  1.10978387e-01  2.71541737e-02\n",
      "  6.76676780e-02  5.04965559e-02 -2.29080208e-03 -1.79503579e-03\n",
      " -9.21667442e-02 -7.33829364e-02  1.22517981e-02  6.17306456e-02\n",
      "  1.91551130e-02  4.82745841e-02  1.18233487e-02  5.79690859e-02\n",
      " -8.66608787e-03  8.51515308e-02 -2.23902147e-02  9.43178218e-03\n",
      "  6.43687919e-02 -9.25265439e-03  1.24638099e-02 -2.56714616e-02\n",
      " -4.47991565e-02  1.40847238e-02  2.77433004e-02 -2.63500720e-01\n",
      "  4.34072129e-02 -8.61062086e-04  6.59302473e-02  4.17902954e-02\n",
      "  7.23481830e-03  1.47360293e-02 -4.39575836e-02  3.75306234e-02\n",
      " -2.76609045e-02 -2.17410307e-02  3.10591757e-02  8.34612101e-02\n",
      "  1.15440814e-02 -9.49848164e-03  3.52460169e-03  6.88833073e-02\n",
      " -4.77905497e-02  1.38955442e-02 -3.53022180e-02 -2.43767314e-02\n",
      "  1.48775252e-02  1.61581188e-01 -3.83679233e-02  4.15152088e-02\n",
      " -5.81103340e-02  5.25889266e-03 -8.00779462e-02  1.83933415e-02\n",
      "  2.24029506e-03 -3.10359709e-03  2.92857345e-02  5.83268851e-02\n",
      " -1.38767739e-03  1.04072364e-02  6.47917092e-02 -1.88139628e-03\n",
      "  5.10252416e-02 -5.31181740e-03  1.76140741e-02  2.58924533e-02\n",
      "  4.55179624e-02 -2.90679634e-02 -6.72976971e-02  8.55220705e-02\n",
      "  2.07802877e-02 -2.08205525e-02 -3.62562351e-02 -5.28421886e-02\n",
      "  3.06857508e-02  2.06403527e-03  3.63261346e-03  1.67174488e-02\n",
      " -2.42664441e-02  3.48551348e-02  1.96489692e-02 -3.97949889e-02\n",
      " -1.01860436e-02 -1.08828330e-02  1.37985311e-02  1.88212022e-02\n",
      " -4.81819510e-02 -2.62104557e-03  5.91697134e-02 -4.65232767e-02]\n",
      "(384,)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(huggingface_embeddings.embed_query(final_document[0].page_content)))\n",
    "print(np.array(huggingface_embeddings.embed_query(final_document[0].page_content)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b8cb2db-3c3d-4939-b056-18790e008147",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore=FAISS.from_documents(final_document[:150],huggingface_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8bbf460-d3c1-4640-8e7f-1269695c6008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 1: The Transformer - model architecture.\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
      "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
      "respectively.\n",
      "3.1 Encoder and Decoder Stacks\n",
      "Encoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\n",
      "sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "wise fully connected feed-forward network. We employ a residual connection [ 11] around each of\n",
      "the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\n",
      "LayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\n",
      "itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\n",
      "layers, produce outputs of dimension dmodel = 512 .\n"
     ]
    }
   ],
   "source": [
    "query=\"What is Transformer?\"\n",
    "relevant_docments=vectorstore.similarity_search(query)\n",
    "\n",
    "print(relevant_docments[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "084a8acd-a851-4e9e-b76f-c358b200cffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore=FAISS.from_documents(final_document[:150],huggingface_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67a7e844-43fc-40da-ad76-3a1d824520d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags=['FAISS', 'HuggingFaceBgeEmbeddings'] vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000020E951BED10> search_kwargs={'k': 3}\n"
     ]
    }
   ],
   "source": [
    "retriever=vectorstore.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":3})\n",
    "print(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9f3747e-df41-4ee7-be8c-70f4bf5789cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = os.getenv(\"hf_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "490a8210-83c4-4d52-b4ea-f28dcf60edc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\gen_ai_nlp\\Langchain\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEndpoint`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "hf1=HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-v0.1\",\n",
    "    model_kwargs={\"temperature\":0.1,\"max_length\":500}\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc11094d-f2b5-4b89-90d6-4cabdbd50de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is transformer model architecture?\\n\\nTransformer is a neural network architecture that is used for natural language processing (NLP) tasks. It was introduced in 2017 by Google researchers and has since become one of the most popular architectures for NLP.\\n\\nThe transformer model is composed of two main components: an encoder and a decoder. The encoder takes as input a sequence of words and outputs a representation of the words in the sequence. The decoder takes as input the representation of'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"what is transformer model architecture?\"\n",
    "hf1.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d265efb9-f8fc-4121-bfbd-bd436e89e245",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=\"\"\"\n",
    "Use the following piece of context to answer the question asked.\n",
    "Please try to provide the answer only based on the context\n",
    "\n",
    "{context}\n",
    "Question:{question}\n",
    "\n",
    "Helpful Answers:\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30189dce-b46b-4546-8fb1-e0eecf40a416",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(template=prompt_template,input_variables=[\"context\",\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94d9db62-41d0-4232-ab61-72552a5077c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrievalQA=RetrievalQA.from_chain_type(\n",
    "    llm=hf1,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\":prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d621dd3f-3b85-444f-8a1a-f0d8a7821383",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"\"\"How Transformer model works?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77932a86-45c4-4f08-8856-2fed2694f300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Use the following piece of context to answer the question asked.\n",
      "Please try to provide the answer only based on the context\n",
      "\n",
      "Figure 1: The Transformer - model architecture.\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
      "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
      "respectively.\n",
      "3.1 Encoder and Decoder Stacks\n",
      "Encoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\n",
      "sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "wise fully connected feed-forward network. We employ a residual connection [ 11] around each of\n",
      "the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\n",
      "LayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\n",
      "itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\n",
      "layers, produce outputs of dimension dmodel = 512 .\n",
      "\n",
      "best models from the literature. We show that the Transformer generalizes well to\n",
      "other tasks by applying it successfully to English constituency parsing both with\n",
      "large and limited training data.\n",
      "∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\n",
      "has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
      "attention and the parameter-free position representation and became the other person involved in nearly every\n",
      "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
      "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\n",
      "efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\n",
      "\n",
      "textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "language modeling tasks [34].\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
      "entirely on self-attention to compute representations of its input and output without using sequence-\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "3 Model Architecture\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\n",
      "Here, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\n",
      "of continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\n",
      "Question:How Transformer model works?\n",
      "\n",
      "Helpful Answers:\n",
      " 1.\n",
      " 2.\n",
      " 3.\n",
      " 4.\n",
      " 5.\n",
      " 6.\n",
      " 7.\n",
      " 8.\n",
      " 9.\n",
      " 10.\n",
      " 11.\n",
      " 12.\n",
      " 13.\n",
      " 14.\n",
      " 15.\n",
      " 16.\n",
      " 17.\n",
      " 18.\n",
      " 19.\n",
      " 20.\n",
      " 21.\n",
      " 22.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = retrievalQA.invoke({\"query\": query})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0f2f2b-92a0-43f3-89e0-0e487709c669",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
